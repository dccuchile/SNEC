{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "REALLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj9ptIGwtvUt",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "# !pip install --upgrade gensim\n",
        "# !pip install transformers\n",
        "# !pip install -U sentence-transformers\n",
        "# !pip install kornia\n",
        "# # # !pip install \"torch==1.7.0\"\n",
        "# !pip install flair\n",
        "# !pip install captum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoZY0qQktvU0",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, hamming_loss, confusion_matrix\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import time\n",
        "# from kornia.losses import BinaryFocalLossWithLogits\n",
        "\n",
        "# from captum.attr import LayerIntegratedGradients\n",
        "# from captum.attr import visualization as viz\n",
        "\n",
        "# import flair\n",
        "# from flair.data import Sentence\n",
        "# from flair.embeddings import (\n",
        "#     TransformerDocumentEmbeddings, \n",
        "#     SentenceTransformerDocumentEmbeddings, \n",
        "#     FlairEmbeddings, \n",
        "#     StackedEmbeddings, \n",
        "#     CharacterEmbeddings, \n",
        "#     DocumentPoolEmbeddings,\n",
        "#     WordEmbeddings,\n",
        "#     TransformerWordEmbeddings)\n",
        "\n",
        "import warnings\n",
        "import traceback\n",
        "\n",
        "import logging\n",
        "# logger = logging.getLogger('flair')\n",
        "# logger.setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypzJyW9AtvU1",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "232e930f-b257-48aa-cf1a-057fcf028cf0"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Jun 17 15:35:46 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.119.04   Driver Version: 450.119.04   CUDA Version: 11.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  GeForce RTX 2080    Off  | 00000000:03:00.0 Off |                  N/A |\n",
            "| 26%   61C    P2   101W / 260W |   5274MiB /  7982MiB |     45%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   1  GeForce RTX 2080    Off  | 00000000:05:00.0 Off |                  N/A |\n",
            "|  0%   43C    P8    27W / 260W |      3MiB /  7982MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A     40329      C   ...unoz/anaconda3/bin/python     5271MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6G2-fISXQyfr",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "738f84f5-01fb-4eba-e485-99fbaa70d9df"
      },
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda:1\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(1))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "torch.backends.cudnn.enabled=False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 2 GPU(s) available.\n",
            "We will use the GPU: GeForce RTX 2080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RonQHx7OJfXj"
      },
      "source": [
        "## Preparing Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUgSQCJlNl2r"
      },
      "source": [
        "### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdipkFQDJi6p",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "categories_number_words = {\n",
        "        1: \"Apoyo Pedagógico en asignaturas\",\n",
        "        3: \"Apoyo pedagógico personal\",\n",
        "        4: \"Tutoría entre pares\",\n",
        "        7: \"Hacer a la familia partícipe del proceso\",\n",
        "        8: \"Apoyo psicóloga(o)\",\n",
        "        9: \"Apoyo fonoaudióloga(o)\",\n",
        "        10: \"Apoyo Educador(a) Diferencial\",\n",
        "        11: \"Apoyo Kinesióloga(o)\",\n",
        "        12: \"Apoyo Médico General\",\n",
        "        13: \"Apoyo Terapeuta Ocupacional\",\n",
        "        14: \"Control Neurólogo\",\n",
        "        15: \"Apoyo Interdisciplinario\",\n",
        "        16: \"Adecuación curricular de acceso\",\n",
        "        17: \"Adecuación curricular de objetivos\"\n",
        "    }\n",
        "categories_words_number = {v: k for k, v in categories_number_words.items()}\n",
        "\n",
        "diagnoses_codes = {\n",
        "    \"Trastorno específico del lenguaje\": 0,\n",
        "    \"Trastorno por déficit atencional\": 1,\n",
        "    \"Dificultad específica de aprendizaje\": 2,\n",
        "    \"Discapacidad intelectual\": 3,\n",
        "    \"Discapacidad visual\": 4,\n",
        "    \"Trastorno del espectro autista\": 5,\n",
        "    \"Discapacidad auditiva - Hipoacusia\": 6,\n",
        "    \"Funcionamiento intelectual limítrofe\": 7,\n",
        "    \"Síndrome de Down\": 8,\n",
        "    \"Trastorno motor\": 9,\n",
        "    \"Multidéficit\": 10,\n",
        "    \"Retraso global del desarrollo\": 11\n",
        "}\n",
        "\n",
        "diagnoses_keys = list(diagnoses_codes.keys())\n",
        "\n",
        "def transform_diag_to_array(code):\n",
        "    arr = np.zeros(len(diagnoses_keys), dtype=int)\n",
        "    for (index, label) in enumerate(diagnoses_keys):\n",
        "        if diagnoses_codes[label]==code:\n",
        "            arr[index] = 1\n",
        "    return arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69bH3NpmMBFs",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "train_dataset = pd.read_csv('/research/jamunoz/datasets/train_ds.csv', keep_default_na=False)\n",
        "val_dataset = pd.read_csv('/research/jamunoz/datasets/val_ds.csv', keep_default_na=False)\n",
        "test_dataset = pd.read_csv('/research/jamunoz/datasets/test_ds.csv', keep_default_na=False)\n",
        "# train_dataset = pd.read_csv('gdrive/My Drive/magister/train_ds.csv', keep_default_na=False)\n",
        "# val_dataset = pd.read_csv('gdrive/My Drive/magister/val_ds.csv', keep_default_na=False)\n",
        "# test_dataset = pd.read_csv('gdrive/My Drive/magister/test_ds.csv', keep_default_na=False)\n",
        "\n",
        "\n",
        "# Add OHE diagnosis\n",
        "train_OHE_diags = []\n",
        "for diag in train_dataset['Encoded Diagnosis']:\n",
        "    train_OHE_diags.append(transform_diag_to_array(diag))\n",
        "temp_train_diags_df = pd.DataFrame(train_OHE_diags, columns=diagnoses_keys)\n",
        "train_dataset = pd.concat([train_dataset, temp_train_diags_df], axis=1)\n",
        "\n",
        "val_OHE_diags = []\n",
        "for diag in val_dataset['Encoded Diagnosis']:\n",
        "    val_OHE_diags.append(transform_diag_to_array(diag))\n",
        "temp_val_diags_df = pd.DataFrame(val_OHE_diags, columns=diagnoses_keys)\n",
        "val_dataset = pd.concat([val_dataset, temp_val_diags_df], axis=1)\n",
        "\n",
        "test_OHE_diags = []\n",
        "for diag in test_dataset['Encoded Diagnosis']:\n",
        "    test_OHE_diags.append(transform_diag_to_array(diag))\n",
        "temp_test_diags_df = pd.DataFrame(test_OHE_diags, columns=diagnoses_keys)\n",
        "test_dataset = pd.concat([test_dataset, temp_test_diags_df], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CsWWAVbMHrI",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "# y_keys = list(strat_present.keys())\n",
        "Y_KEYS = list(categories_words_number.keys())\n",
        "\n",
        "# df = pd.DataFrame(data=new_dataset_to_export)\n",
        "# X = df\n",
        "# Y = df[y_keys]\n",
        "X_train = train_dataset.drop(Y_KEYS, axis=1)\n",
        "Y_train = train_dataset[Y_KEYS]\n",
        "X_val = val_dataset.drop(Y_KEYS, axis=1)\n",
        "Y_val = val_dataset[Y_KEYS]\n",
        "X_test = test_dataset.drop(Y_KEYS, axis=1)\n",
        "Y_test = test_dataset[Y_KEYS]\n",
        "\n",
        "strats_amounts = {\n",
        "              'Adecuación curricular de acceso': 2264,\n",
        "              'Hacer a la familia partícipe del proceso': 2048,\n",
        "              'Apoyo Interdisciplinario': 1441, \n",
        "              'Apoyo Educador(a) Diferencial': 1311,\n",
        "              'Apoyo pedagógico personal': 1240,\n",
        "              'Apoyo fonoaudióloga(o)': 378,\n",
        "              'Apoyo psicóloga(o)': 588,\n",
        "              'Apoyo Terapeuta Ocupacional': 153,\n",
        "              'Tutoría entre pares': 350,\n",
        "              'Control Neurólogo': 63,\n",
        "              'Apoyo Médico General': 64,\n",
        "              'Apoyo Kinesióloga(o)': 32,\n",
        "              'Adecuación curricular de objetivos': 281,\n",
        "              'Apoyo Pedagógico en asignaturas': 1314\n",
        "}\n",
        "most_unbalanced_strategies = [strategy for strategy in Y_KEYS if (\n",
        "    strats_amounts[strategy] < (len(X_train) + len(X_val) + len(X_test))*0.15 or strats_amounts[strategy] > (len(X_train) + len(X_val) + len(X_test))*0.85)]\n",
        "less_unbalanced_strategies = [strategy for strategy in Y_KEYS if strategy not in most_unbalanced_strategies]\n",
        "only_one_strat = [Y_KEYS[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONu-YYe8PIlF"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9skDES6aId3",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nymd7231RqJJ",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "class AllJoinedObservationsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      data_row = self.data.iloc[idx]\n",
        "      labels =  data_row[Y_KEYS]\n",
        "\n",
        "      tensor_labels = torch.tensor(labels, dtype=torch.int)\n",
        "      tensor_diags = torch.tensor(data_row[diagnoses_keys], dtype=torch.int)\n",
        "\n",
        "      all_tokens = tokenizer.encode(data_row['All perceptions'],\n",
        "                                    add_special_tokens=False,\n",
        "                                    max_length=tokenizer.model_max_length,\n",
        "                                    padding='max_length',\n",
        "                                    truncation=True,\n",
        "                                    return_tensors=\"pt\")\n",
        "\n",
        "      sne_tokens = tokenizer.encode(data_row['Special Education Teacher Perceptions'],\n",
        "                                    add_special_tokens=False,\n",
        "                                    max_length=tokenizer.model_max_length,\n",
        "                                    padding='max_length',\n",
        "                                    truncation=True,\n",
        "                                    return_tensors=\"pt\")\n",
        "\n",
        "      st_tokens = tokenizer.encode(data_row['Speech Therapist Perceptions'],\n",
        "                                    add_special_tokens=False,\n",
        "                                    max_length=tokenizer.model_max_length,\n",
        "                                    padding='max_length',\n",
        "                                    truncation=True,\n",
        "                                    return_tensors=\"pt\")\n",
        "\n",
        "      m_tokens = tokenizer.encode(data_row['Medical Perceptions'],\n",
        "                                    add_special_tokens=False,\n",
        "                                    max_length=tokenizer.model_max_length,\n",
        "                                    padding='max_length',\n",
        "                                    truncation=True,\n",
        "                                    return_tensors=\"pt\")\n",
        "\n",
        "      p_tokens = tokenizer.encode(data_row['Psychologist Perceptions'],\n",
        "                                    add_special_tokens=False,\n",
        "                                    max_length=tokenizer.model_max_length,\n",
        "                                    padding='max_length',\n",
        "                                    truncation=True,\n",
        "                                    return_tensors=\"pt\")\n",
        "\n",
        "      dict_to_return = dict(\n",
        "          all_tokens=all_tokens,\n",
        "          sne_tokens=sne_tokens,\n",
        "          st_tokens=st_tokens,\n",
        "          p_tokens=p_tokens,\n",
        "          m_tokens=m_tokens,\n",
        "          labels=tensor_labels,\n",
        "          diagnostics=tensor_diags\n",
        "      )\n",
        "\n",
        "      return dict_to_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kXynftO6NNr"
      },
      "source": [
        "### Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m96W-bR1Is2",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "TRAIN_BATCH_SIZE=1\n",
        "\n",
        "def my_collate1(batches):\n",
        "  modified_batches = []\n",
        "  for batch in batches:\n",
        "    batch_dict = {}\n",
        "    for key, value in batch.items():\n",
        "      batch_dict[key] = value\n",
        "    modified_batches.append(batch_dict)\n",
        "  return modified_batches\n",
        "\n",
        "transformed_train_dataset=AllJoinedObservationsDataset(\n",
        "    train_dataset)\n",
        "\n",
        "transformed_val_dataset=AllJoinedObservationsDataset(\n",
        "    val_dataset)\n",
        "\n",
        "transformed_test_dataset=AllJoinedObservationsDataset(\n",
        "    test_dataset)\n",
        "\n",
        "train_data_loader=DataLoader(\n",
        "    transformed_train_dataset,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    collate_fn=my_collate1)\n",
        "\n",
        "val_data_loader=DataLoader(\n",
        "    transformed_val_dataset,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    collate_fn=my_collate1)\n",
        "\n",
        "test_data_loader=DataLoader(\n",
        "    transformed_test_dataset,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    collate_fn=my_collate1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOLV6bNeo9Fu"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxgILmMRpASj",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "import statistics\n",
        "\n",
        "def get_results(targets, outputs):\n",
        "  TN = 0\n",
        "  TP = 0\n",
        "  FP = 0\n",
        "  FN = 0\n",
        "  for (i, output) in enumerate(outputs):\n",
        "    if output==0:\n",
        "      if targets[i]==0:\n",
        "        TN += 1\n",
        "      else:\n",
        "        FN += 1\n",
        "    else:\n",
        "      if targets[i]==1:\n",
        "        TP += 1\n",
        "      else:\n",
        "        FP += 1\n",
        "  return TP, TN, FP, FN\n",
        "\n",
        "def findMinDiff(arr):\n",
        "    n = len(arr)\n",
        "    arr = sorted(arr)\n",
        "    diff = 0.5\n",
        "    for i in range(n-1):\n",
        "        if arr[i+1] - arr[i] > 0 and arr[i+1] - arr[i] < diff:\n",
        "            diff = arr[i+1] - arr[i]\n",
        "    return diff\n",
        "\n",
        "def get_thresholds(targets, outputs):\n",
        "  best_thresholds = []\n",
        "  for i in range(len(outputs[0])):\n",
        "    real_preds = outputs[:, i]\n",
        "    trues = targets[:, i]\n",
        "    max_g = 0\n",
        "#     max_f1 = 0\n",
        "    delta_threshold = 0.0001 # findMinDiff(real_preds)*0.9\n",
        "    positive_ratio = sum(trues)/len(trues)\n",
        "#     print('pr: ', positive_ratio)\n",
        "    if positive_ratio > 0.6:\n",
        "      local_best = 0\n",
        "      curr_threshold = min(real_preds)\n",
        "#       print(curr_threshold)\n",
        "      while curr_threshold < 1:\n",
        "        preds = [1 if pred > curr_threshold else 0 for pred in real_preds]\n",
        "        tp, tn, fp, fn = get_results(trues, preds)\n",
        "        recall = tp/(tp+fn)\n",
        "        specificity = tn/(tn+fp)\n",
        "        g_mean = np.sqrt(recall*specificity)\n",
        "#         f1 = f1_score(trues, preds)\n",
        "#         print(f1, max_f1, curr_threshold, local_best, tp, tn, fp, fn)\n",
        "        if tp < tn:\n",
        "          break\n",
        "        if g_mean > max_g:\n",
        "#         if f1 > max_f1:\n",
        "          max_g = g_mean\n",
        "#           max_f1 = f1\n",
        "          local_best = curr_threshold\n",
        "        curr_threshold += delta_threshold\n",
        "      best_thresholds.append(local_best)\n",
        "    elif positive_ratio < 0.4:\n",
        "      local_best = 1\n",
        "      curr_threshold = max(real_preds)\n",
        "      while curr_threshold > 0:\n",
        "        preds = [1 if pred > curr_threshold else 0 for pred in real_preds]\n",
        "        tp, tn, fp, fn = get_results(trues, preds)\n",
        "        recall = tp/(tp+fn)\n",
        "        specificity = tn/(tn+fp)\n",
        "        g_mean = np.sqrt(recall*specificity)\n",
        "#         f1 = f1_score(trues, preds)\n",
        "        if tn < tp:\n",
        "          break\n",
        "        if g_mean > max_g:\n",
        "#         if f1 > max_f1:\n",
        "          max_g = g_mean\n",
        "#           max_f1 = f1\n",
        "          local_best = curr_threshold\n",
        "        curr_threshold -= delta_threshold\n",
        "      best_thresholds.append(local_best)\n",
        "    else:\n",
        "      local_best = 0.5\n",
        "      best_thresholds.append(local_best)\n",
        "  return best_thresholds\n",
        "\n",
        "def get_individual_threshold(target, output):\n",
        "    real_preds = output\n",
        "    trues = target\n",
        "    max_g = 0\n",
        "    # max_f1 = 0\n",
        "    delta_threshold = 0.0001 # findMinDiff(real_preds)*0.9\n",
        "    positive_ratio = sum(trues)/len(trues)\n",
        "#     print('pr: ', positive_ratio)\n",
        "    if positive_ratio > 0.5:\n",
        "      local_best = 0\n",
        "      curr_threshold = min(real_preds)\n",
        "#       print(curr_threshold)\n",
        "      while curr_threshold < 1:\n",
        "        preds = [1 if pred > curr_threshold else 0 for pred in real_preds]\n",
        "        tp, tn, fp, fn = get_results(trues, preds)\n",
        "        recall = tp/(tp+fn)\n",
        "        specificity = tn/(tn+fp)\n",
        "        g_mean = np.sqrt(recall*specificity)\n",
        "        # f1 = f1_score(trues, preds)\n",
        "#         print(f1, max_f1, curr_threshold, local_best, tp, tn, fp, fn)\n",
        "        if tp < tn:\n",
        "          break\n",
        "        if g_mean > max_g:\n",
        "        # if f1 > max_f1:\n",
        "          max_g = g_mean\n",
        "          # max_f1 = f1\n",
        "          local_best = curr_threshold\n",
        "        curr_threshold += delta_threshold\n",
        "      return local_best\n",
        "    else:\n",
        "      local_best = 1\n",
        "      curr_threshold = max(real_preds)\n",
        "      while curr_threshold > 0:\n",
        "        preds = [1 if pred > curr_threshold else 0 for pred in real_preds]\n",
        "        tp, tn, fp, fn = get_results(trues, preds)\n",
        "        recall = tp/(tp+fn)\n",
        "        specificity = tn/(tn+fp)\n",
        "        g_mean = np.sqrt(recall*specificity)\n",
        "        # f1 = f1_score(trues, preds)\n",
        "        if tn < tp:\n",
        "          break\n",
        "        if g_mean > max_g:\n",
        "        # if f1 > max_f1:\n",
        "          max_g = g_mean\n",
        "          # max_f1 = f1\n",
        "          local_best = curr_threshold\n",
        "        curr_threshold -= delta_threshold\n",
        "      return local_best\n",
        "    # else:\n",
        "    #   local_best = 0.5\n",
        "    #   return local_best"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gSsCmTCo_Cd",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "def loss_fun(outputs, targets):\n",
        "    loss = nn.BCEWithLogitsLoss()\n",
        "    # loss = BinaryFocalLossWithLogits(alpha=0.25, reduction='mean')\n",
        "    try:\n",
        "      return loss(outputs, targets)\n",
        "    except Exception:\n",
        "      print(outputs, targets)\n",
        "      traceback.print_exc()\n",
        "    # return nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "def individual_evaluation(target, predicted):\n",
        "  individual = {}\n",
        "  for i in range(len(target[0])):\n",
        "    temp_t = target[:, i]\n",
        "    temp_p = predicted[:, i]\n",
        "    diction = dict(\n",
        "        accuracy=accuracy_score(temp_t, temp_p),\n",
        "        f1=f1_score(temp_t, temp_p)\n",
        "    )\n",
        "    individual[str(i)] = diction\n",
        "  return individual\n",
        "\n",
        "def evaluate(target, predicted):\n",
        "    thresholds = get_thresholds(target, predicted)\n",
        "    print('thresholds: ', thresholds)\n",
        "    true_predicted = np.array([[1 if val > thresholds[i] else 0 for (i, val) in enumerate(pred)] for pred in predicted])\n",
        "    accuracy = accuracy_score(target, true_predicted)\n",
        "    macro_f1 = f1_score(target, true_predicted, average='macro')\n",
        "    micro_f1 = f1_score(target, true_predicted, average='micro')\n",
        "    weighted_f1 = f1_score(target, true_predicted, average='weighted')\n",
        "    hl = hamming_loss(target, true_predicted)\n",
        "    js = jaccard_score(target, true_predicted)\n",
        "    macro_js = jaccard_score(target, true_predicted, average=\"macro\")\n",
        "    micro_js = jaccard_score(target, true_predicted, average=\"micro\")\n",
        "    individual = individual_evaluation(target, true_predicted)\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"jaccard_score_average\": js,\n",
        "        \"jaccard_score_macro\": macro_js,\n",
        "        \"jaccard_score_micro\": micro_js,\n",
        "        \"macro-f1\": macro_f1,\n",
        "        \"micro-f1\": micro_f1,\n",
        "        \"Hamming Loss\": hl,\n",
        "        \"Individual\": individual\n",
        "    }\n",
        "\n",
        "def individual_evaluation(target, predicted):\n",
        "    threshold = get_individual_threshold(target, predicted)\n",
        "    print('threshold: ',threshold)\n",
        "    true_predicted = np.array([1 if val > threshold else 0 for val in predicted])\n",
        "    default_true_predicted = np.array([1 if val > 0.5 else 0 for val in predicted])\n",
        "    accuracy = accuracy_score(target, true_predicted)\n",
        "    f1 = f1_score(target, true_predicted)\n",
        "    tp, tn, fp, fn = get_results(target, true_predicted)\n",
        "    recall = tp/(tp+fn)\n",
        "    specificity = tn/(tn+fp)\n",
        "    pr = sum(target)/len(target)\n",
        "\n",
        "    default_accuracy = accuracy_score(target, default_true_predicted)\n",
        "    default_f1 = f1_score(target, default_true_predicted)\n",
        "    tp, tn, fp, fn = get_results(target, default_true_predicted)\n",
        "    default_recall = tp/(tp+fn)\n",
        "    default_specificity = tn/(tn+fp)\n",
        "    return {\n",
        "        \"Positive Rate\": pr,\n",
        "        \"threshold\": threshold[0],\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "        \"recall\": recall,\n",
        "        \"specificity\": specificity,\n",
        "        \"default_accuracy\": default_accuracy,\n",
        "        \"default_f1\": default_f1,\n",
        "        \"default_recall\": default_recall,\n",
        "        \"default_specificity\": default_specificity,\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwyMs7HnU3px",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "def individual_test(target, predicted, threshold):\n",
        "    true_predicted = np.array([1 if val > threshold else 0 for val in predicted])\n",
        "    default_true_predicted = np.array([1 if val > 0.5 else 0 for val in predicted])\n",
        "    accuracy = accuracy_score(target, true_predicted)\n",
        "    f1 = f1_score(target, true_predicted)\n",
        "    tp, tn, fp, fn = get_results(target, true_predicted)\n",
        "    recall = tp/(tp+fn)\n",
        "    specificity = tn/(tn+fp)\n",
        "    pr = sum(target)/len(target)\n",
        "\n",
        "    default_accuracy = accuracy_score(target, default_true_predicted)\n",
        "    default_f1 = f1_score(target, default_true_predicted)\n",
        "    tp, tn, fp, fn = get_results(target, default_true_predicted)\n",
        "    default_recall = tp/(tp+fn)\n",
        "    default_specificity = tn/(tn+fp)\n",
        "    return {\n",
        "        \"Positive Rate\": pr,\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "        \"recall\": recall,\n",
        "        \"specificity\": specificity,\n",
        "        \"default_accuracy\": default_accuracy,\n",
        "        \"default_f1\": default_f1,\n",
        "        \"default_recall\": default_recall,\n",
        "        \"default_specificity\": default_specificity,\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X__QjVj_tqcO",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "def individual_eval_loop_fun1(data_loader, model, device, label_index=0):\n",
        "    model.eval()\n",
        "    fin_targets = []\n",
        "    fin_outputs = []\n",
        "    losses = []\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "        text = [Sentence(data[\"all_perceptions\"]) for data in batch]\n",
        "        labels = [data[\"labels\"][label_index] for data in batch]\n",
        "        targets = []\n",
        "        if len(labels) > 1:\n",
        "            for label_set in labels:\n",
        "              miniset = []\n",
        "              for label in label_set:\n",
        "                miniset.append(torch.tensor([label]))\n",
        "              targets.append(torch.stack(miniset))\n",
        "        else:\n",
        "            miniset = [torch.tensor([labels[0]])]\n",
        "            targets.append(torch.stack(miniset))\n",
        "        diagnostics = [data[\"diagnostics\"] for data in batch]\n",
        "\n",
        "        # text = torch.cat(text)\n",
        "        targets = torch.cat(targets)\n",
        "        diagnostics = torch.cat(diagnostics)\n",
        "\n",
        "        # ids = text.to(device, dtype=torch.long)\n",
        "        # mask = mask.to(device, dtype=torch.long)\n",
        "        # token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        targets = targets.to(device, dtype=torch.float)\n",
        "        diagnostics = diagnostics.to(device, dtype=torch.long)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = torch.stack([model(sentence_inp=text, diagnostics=diagnostics)])\n",
        "            loss = loss_fun(outputs, targets)\n",
        "            losses.append(loss.item())\n",
        "            \n",
        "        outputs = torch.sigmoid(outputs)\n",
        "        fin_targets.append(targets.cpu().detach().numpy())\n",
        "        fin_outputs.append(outputs.cpu().detach().numpy())\n",
        "    return np.concatenate(fin_outputs), np.concatenate(fin_targets), losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO5SwlAgzAvB",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "def individual_test_loop_fun1(data_loader, model, device, label_index=0):\n",
        "    model.eval()\n",
        "    fin_targets = []\n",
        "    fin_outputs = []\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "        text = [Sentence(data[\"all_perceptions\"]) for data in batch]\n",
        "        labels = [data[\"labels\"][label_index] for data in batch]\n",
        "        targets = []\n",
        "        if len(labels) > 1:\n",
        "            for label_set in labels:\n",
        "              miniset = []\n",
        "              for label in label_set:\n",
        "                miniset.append(torch.tensor([label]))\n",
        "              targets.append(torch.stack(miniset))\n",
        "        else:\n",
        "            miniset = [torch.tensor([labels[0]])]\n",
        "            targets.append(torch.stack(miniset))\n",
        "        diagnostics = [data[\"diagnostics\"] for data in batch]\n",
        "\n",
        "        # text = torch.cat(text)\n",
        "        targets = torch.cat(targets)\n",
        "        diagnostics = torch.cat(diagnostics)\n",
        "\n",
        "        # ids = text.to(device, dtype=torch.long)\n",
        "        # mask = mask.to(device, dtype=torch.long)\n",
        "        # token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        targets = targets.to(device, dtype=torch.float)\n",
        "        diagnostics = diagnostics.to(device, dtype=torch.long)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = torch.stack([model(sentence_inp=text, diagnostics=diagnostics)])\n",
        "            \n",
        "        outputs = torch.sigmoid(outputs)\n",
        "        fin_targets.append(targets.cpu().detach().numpy())\n",
        "        fin_outputs.append(outputs.cpu().detach().numpy())\n",
        "    return np.concatenate(fin_outputs), np.concatenate(fin_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6rXKHuhlRZS"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0JLVz_ulRZT",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "import statistics\n",
        "\n",
        "def get_results(targets, outputs):\n",
        "  TN = 0\n",
        "  TP = 0\n",
        "  FP = 0\n",
        "  FN = 0\n",
        "  for (i, output) in enumerate(outputs):\n",
        "    if output==0:\n",
        "      if targets[i]==0:\n",
        "        TN += 1\n",
        "      else:\n",
        "        FN += 1\n",
        "    else:\n",
        "      if targets[i]==1:\n",
        "        TP += 1\n",
        "      else:\n",
        "        FP += 1\n",
        "  return TP, TN, FP, FN\n",
        "\n",
        "def findMinDiff(arr):\n",
        "    n = len(arr)\n",
        "    arr = sorted(arr)\n",
        "    diff = 0.5\n",
        "    for i in range(n-1):\n",
        "        if arr[i+1] - arr[i] > 0 and arr[i+1] - arr[i] < diff:\n",
        "            diff = arr[i+1] - arr[i]\n",
        "    return diff\n",
        "\n",
        "def get_thresholds(targets, outputs):\n",
        "  best_thresholds = []\n",
        "  for i in range(len(outputs[0])):\n",
        "    real_preds = outputs[:, i]\n",
        "    trues = targets[:, i]\n",
        "    max_g = 0\n",
        "#     max_f1 = 0\n",
        "    delta_threshold = 0.0001 # findMinDiff(real_preds)*0.9\n",
        "    positive_ratio = sum(trues)/len(trues)\n",
        "#     print('pr: ', positive_ratio)\n",
        "    if positive_ratio > 0.6:\n",
        "      local_best = 0\n",
        "      curr_threshold = min(real_preds)\n",
        "#       print(curr_threshold)\n",
        "      while curr_threshold < 1:\n",
        "        preds = [1 if pred > curr_threshold else 0 for pred in real_preds]\n",
        "        tp, tn, fp, fn = get_results(trues, preds)\n",
        "        recall = tp/(tp+fn)\n",
        "        specificity = tn/(tn+fp)\n",
        "        g_mean = np.sqrt(recall*specificity)\n",
        "#         f1 = f1_score(trues, preds)\n",
        "#         print(f1, max_f1, curr_threshold, local_best, tp, tn, fp, fn)\n",
        "        if tp < tn:\n",
        "          break\n",
        "        if g_mean > max_g:\n",
        "#         if f1 > max_f1:\n",
        "          max_g = g_mean\n",
        "#           max_f1 = f1\n",
        "          local_best = curr_threshold\n",
        "        curr_threshold += delta_threshold\n",
        "      best_thresholds.append(local_best)\n",
        "    elif positive_ratio < 0.4:\n",
        "      local_best = 1\n",
        "      curr_threshold = max(real_preds)\n",
        "      while curr_threshold > 0:\n",
        "        preds = [1 if pred > curr_threshold else 0 for pred in real_preds]\n",
        "        tp, tn, fp, fn = get_results(trues, preds)\n",
        "        recall = tp/(tp+fn)\n",
        "        specificity = tn/(tn+fp)\n",
        "        g_mean = np.sqrt(recall*specificity)\n",
        "#         f1 = f1_score(trues, preds)\n",
        "        if tn < tp:\n",
        "          break\n",
        "        if g_mean > max_g:\n",
        "#         if f1 > max_f1:\n",
        "          max_g = g_mean\n",
        "#           max_f1 = f1\n",
        "          local_best = curr_threshold\n",
        "        curr_threshold -= delta_threshold\n",
        "      best_thresholds.append(local_best)\n",
        "    else:\n",
        "      local_best = 0.5\n",
        "      best_thresholds.append(local_best)\n",
        "  return best_thresholds\n",
        "\n",
        "def get_individual_threshold(target, output):\n",
        "    real_preds = output\n",
        "    trues = target\n",
        "    max_g = 0\n",
        "    # max_f1 = 0\n",
        "    delta_threshold = 0.0001 # findMinDiff(real_preds)*0.9\n",
        "    positive_ratio = sum(trues)/len(trues)\n",
        "#     print('pr: ', positive_ratio)\n",
        "    if positive_ratio > 0.5:\n",
        "      local_best = 0\n",
        "      curr_threshold = min(real_preds)\n",
        "#       print(curr_threshold)\n",
        "      while curr_threshold < 1:\n",
        "        preds = [1 if pred > curr_threshold else 0 for pred in real_preds]\n",
        "        tp, tn, fp, fn = get_results(trues, preds)\n",
        "        recall = tp/(tp+fn)\n",
        "        specificity = tn/(tn+fp)\n",
        "        g_mean = np.sqrt(recall*specificity)\n",
        "        # f1 = f1_score(trues, preds)\n",
        "#         print(f1, max_f1, curr_threshold, local_best, tp, tn, fp, fn)\n",
        "        if tp < tn:\n",
        "          break\n",
        "        if g_mean > max_g:\n",
        "        # if f1 > max_f1:\n",
        "          max_g = g_mean\n",
        "          # max_f1 = f1\n",
        "          local_best = curr_threshold\n",
        "        curr_threshold += delta_threshold\n",
        "      return local_best\n",
        "    else:\n",
        "      local_best = 1\n",
        "      curr_threshold = max(real_preds)\n",
        "      while curr_threshold > 0:\n",
        "        preds = [1 if pred > curr_threshold else 0 for pred in real_preds]\n",
        "        tp, tn, fp, fn = get_results(trues, preds)\n",
        "        recall = tp/(tp+fn)\n",
        "        specificity = tn/(tn+fp)\n",
        "        g_mean = np.sqrt(recall*specificity)\n",
        "        # f1 = f1_score(trues, preds)\n",
        "        if tn < tp:\n",
        "          break\n",
        "        if g_mean > max_g:\n",
        "        # if f1 > max_f1:\n",
        "          max_g = g_mean\n",
        "          # max_f1 = f1\n",
        "          local_best = curr_threshold\n",
        "        curr_threshold -= delta_threshold\n",
        "      return local_best\n",
        "    # else:\n",
        "    #   local_best = 0.5\n",
        "    #   return local_best"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV9R7Q4elRZU",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "def loss_fun(outputs, targets):\n",
        "    loss = nn.BCEWithLogitsLoss()\n",
        "    # loss = BinaryFocalLossWithLogits(alpha=0.25, reduction='mean')\n",
        "    try:\n",
        "      return loss(outputs, targets)\n",
        "    except Exception:\n",
        "      print(outputs, targets)\n",
        "      traceback.print_exc()\n",
        "    # return nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "def individual_evaluation(target, predicted):\n",
        "  individual = {}\n",
        "  for i in range(len(target[0])):\n",
        "    temp_t = target[:, i]\n",
        "    temp_p = predicted[:, i]\n",
        "    diction = dict(\n",
        "        accuracy=accuracy_score(temp_t, temp_p),\n",
        "        f1=f1_score(temp_t, temp_p)\n",
        "    )\n",
        "    individual[str(i)] = diction\n",
        "  return individual\n",
        "\n",
        "def evaluate(target, predicted):\n",
        "    thresholds = get_thresholds(target, predicted)\n",
        "    print('thresholds: ', thresholds)\n",
        "    true_predicted = np.array([[1 if val > thresholds[i] else 0 for (i, val) in enumerate(pred)] for pred in predicted])\n",
        "    accuracy = accuracy_score(target, true_predicted)\n",
        "    macro_f1 = f1_score(target, true_predicted, average='macro')\n",
        "    micro_f1 = f1_score(target, true_predicted, average='micro')\n",
        "    weighted_f1 = f1_score(target, true_predicted, average='weighted')\n",
        "    hl = hamming_loss(target, true_predicted)\n",
        "    js = jaccard_score(target, true_predicted)\n",
        "    macro_js = jaccard_score(target, true_predicted, average=\"macro\")\n",
        "    micro_js = jaccard_score(target, true_predicted, average=\"micro\")\n",
        "    individual = individual_evaluation(target, true_predicted)\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"jaccard_score_average\": js,\n",
        "        \"jaccard_score_macro\": macro_js,\n",
        "        \"jaccard_score_micro\": micro_js,\n",
        "        \"macro-f1\": macro_f1,\n",
        "        \"micro-f1\": micro_f1,\n",
        "        \"Hamming Loss\": hl,\n",
        "        \"Individual\": individual\n",
        "    }\n",
        "\n",
        "def individual_evaluation(target, predicted):\n",
        "    threshold = get_individual_threshold(target, predicted)\n",
        "    print('threshold: ',threshold)\n",
        "    true_predicted = np.array([1 if val > threshold else 0 for val in predicted])\n",
        "    default_true_predicted = np.array([1 if val > 0.5 else 0 for val in predicted])\n",
        "    accuracy = accuracy_score(target, true_predicted)\n",
        "    f1 = f1_score(target, true_predicted)\n",
        "    tp, tn, fp, fn = get_results(target, true_predicted)\n",
        "    recall = tp/(tp+fn)\n",
        "    specificity = tn/(tn+fp)\n",
        "    pr = sum(target)/len(target)\n",
        "\n",
        "    default_accuracy = accuracy_score(target, default_true_predicted)\n",
        "    default_f1 = f1_score(target, default_true_predicted)\n",
        "    tp, tn, fp, fn = get_results(target, default_true_predicted)\n",
        "    default_recall = tp/(tp+fn)\n",
        "    default_specificity = tn/(tn+fp)\n",
        "    return {\n",
        "        \"Positive Rate\": pr,\n",
        "        \"threshold\": threshold[0],\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "        \"recall\": recall,\n",
        "        \"specificity\": specificity,\n",
        "        \"default_accuracy\": default_accuracy,\n",
        "        \"default_f1\": default_f1,\n",
        "        \"default_recall\": default_recall,\n",
        "        \"default_specificity\": default_specificity,\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJWqxg5slRZW",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "def individual_test(target, predicted, threshold):\n",
        "    true_predicted = np.array([1 if val > threshold else 0 for val in predicted])\n",
        "    default_true_predicted = np.array([1 if val > 0.5 else 0 for val in predicted])\n",
        "    accuracy = accuracy_score(target, true_predicted)\n",
        "    f1 = f1_score(target, true_predicted)\n",
        "    tp, tn, fp, fn = get_results(target, true_predicted)\n",
        "    recall = tp/(tp+fn)\n",
        "    specificity = tn/(tn+fp)\n",
        "    pr = sum(target)/len(target)\n",
        "\n",
        "    default_accuracy = accuracy_score(target, default_true_predicted)\n",
        "    default_f1 = f1_score(target, default_true_predicted)\n",
        "    tp, tn, fp, fn = get_results(target, default_true_predicted)\n",
        "    default_recall = tp/(tp+fn)\n",
        "    default_specificity = tn/(tn+fp)\n",
        "    return {\n",
        "        \"Positive Rate\": pr,\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "        \"recall\": recall,\n",
        "        \"specificity\": specificity,\n",
        "        \"default_accuracy\": default_accuracy,\n",
        "        \"default_f1\": default_f1,\n",
        "        \"default_recall\": default_recall,\n",
        "        \"default_specificity\": default_specificity,\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WUZvwDfeUNH"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHnUFWgDNE3Y",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "class New_Model(nn.Module):\n",
        "\n",
        "  def __init__(self, embedding_model, n_diags):\n",
        "    super(New_Model, self).__init__()\n",
        "\n",
        "    # Pass the flair\n",
        "    self.embedding_model = embedding_model\n",
        "\n",
        "    self.n_diags = n_diags\n",
        "        \n",
        "    self.embedding_model.eval()\n",
        "    self.embedding_model.zero_grad()\n",
        "\n",
        "    self.lstm_output = 100\n",
        "\n",
        "    self.lstm = nn.LSTM(768, self.lstm_output, num_layers=1, bidirectional=True)\n",
        "    self.out = nn.Linear( self.lstm_output*2 + n_diags, 1)\n",
        "\n",
        "  def forward(self, input_ids_diags):\n",
        "    input_ids = input_ids_diags[:,:-1*self.n_diags]\n",
        "    diags = input_ids_diags[:, -1*self.n_diags:]\n",
        "    sequence_output = self.embedding_model(input_ids=input_ids).last_hidden_state\n",
        "\n",
        "    b = sequence_output.transpose(0, 1)\n",
        "    packed_output, (h_t, h_c) = self.lstm(b, )\n",
        "    hidden = torch.cat((h_t[0],h_t[1]),dim=1)\n",
        "    output = torch.cat((hidden, diags), dim=1)\n",
        "    output = self.out(output)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQlIvkA_o2Pu",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "def individual_train_loop_fun1(data_loader, model, optimizer, device, grad_accs, scheduler=None, label_index=0):\n",
        "    model.train()\n",
        "    t0 = time.time()\n",
        "    losses = []\n",
        "    optimizer.zero_grad()\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "        text = [data[\"all_tokens\"] for data in batch]\n",
        "        labels = [data[\"labels\"][label_index] for data in batch]\n",
        "        targets = []\n",
        "        if len(labels) > 1:\n",
        "            for label_set in labels:\n",
        "              miniset = []\n",
        "              for label in label_set:\n",
        "                miniset.append(torch.tensor([label]))\n",
        "              targets.append(torch.stack(miniset))\n",
        "        else:\n",
        "            miniset = [torch.tensor([labels[0]])]\n",
        "            targets.append(torch.stack(miniset))\n",
        "        diagnostics = [data[\"diagnostics\"] for data in batch]\n",
        "\n",
        "        text = torch.cat(text)\n",
        "        text = text.to(device)\n",
        "\n",
        "        # text = torch.cat(text)\n",
        "        targets = torch.cat(targets)\n",
        "        diagnostics = torch.cat(diagnostics)\n",
        "        diagnostics = torch.stack([diagnostics])\n",
        "\n",
        "        # ids = text.to(device, dtype=torch.long)\n",
        "        # mask = mask.to(device, dtype=torch.long)\n",
        "        # token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        targets = targets.to(device, dtype=torch.float)\n",
        "        diagnostics = diagnostics.to(device, dtype=torch.long)\n",
        "\n",
        "        input_ids_diags = torch.cat((text, diagnostics), dim=1)\n",
        "        input_ids_diags = input_ids_diags.to(device)\n",
        "\n",
        "        outputs = model(input_ids_diags=input_ids_diags)\n",
        "        # outputs = torch.cat(torch.unbind(outputs))\n",
        "        loss = loss_fun(outputs, targets)\n",
        "        (loss / grad_accs).backward()\n",
        "        model.float()\n",
        "        if (batch_idx + 1) % grad_accs == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            if scheduler:\n",
        "                scheduler.step()\n",
        "        losses.append(loss.item())\n",
        "        if batch_idx % 250 == 0:\n",
        "            print(\n",
        "                f\"___ batch index = {batch_idx} / {len(data_loader)} ({100*batch_idx / len(data_loader):.2f}%), loss = {np.mean(losses[-10:]):.4f}, time = {time.time()-t0:.2f} seconds ___\")\n",
        "            t0 = time.time()\n",
        "    return losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4PXFBR_0Kr3",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "def individual_eval_loop_fun1(data_loader, model, device, label_index=0):\n",
        "    model.eval()\n",
        "    fin_targets = []\n",
        "    fin_outputs = []\n",
        "    losses = []\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "        text = [data[\"all_tokens\"]  for data in batch]\n",
        "        labels = [data[\"labels\"][label_index] for data in batch]\n",
        "        targets = []\n",
        "        if len(labels) > 1:\n",
        "            for label_set in labels:\n",
        "              miniset = []\n",
        "              for label in label_set:\n",
        "                miniset.append(torch.tensor([label]))\n",
        "              targets.append(torch.stack(miniset))\n",
        "        else:\n",
        "            miniset = [torch.tensor([labels[0]])]\n",
        "            targets.append(torch.stack(miniset))\n",
        "        diagnostics = [data[\"diagnostics\"] for data in batch]\n",
        "\n",
        "        text = torch.cat(text)\n",
        "        text = text.to(device)\n",
        "\n",
        "        # text = torch.cat(text)\n",
        "        targets = torch.cat(targets)\n",
        "        diagnostics = torch.cat(diagnostics)\n",
        "        diagnostics = torch.stack([diagnostics])\n",
        "\n",
        "        # ids = text.to(device, dtype=torch.long)\n",
        "        # mask = mask.to(device, dtype=torch.long)\n",
        "        # token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        targets = targets.to(device, dtype=torch.float)\n",
        "        diagnostics = diagnostics.to(device, dtype=torch.long)\n",
        "\n",
        "        input_ids_diags = torch.cat((text, diagnostics), dim=1)\n",
        "        input_ids_diags = input_ids_diags.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids_diags=input_ids_diags)\n",
        "            loss = loss_fun(outputs, targets)\n",
        "            losses.append(loss.item())\n",
        "            \n",
        "        outputs = torch.sigmoid(outputs)\n",
        "        fin_targets.append(targets.cpu().detach().numpy())\n",
        "        fin_outputs.append(outputs.cpu().detach().numpy())\n",
        "    return np.concatenate(fin_outputs), np.concatenate(fin_targets), losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBuUOJQKSX3w",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "def individual_test_loop_fun1(data_loader, model, device, label_index=0):\n",
        "    model.eval()\n",
        "    fin_targets = []\n",
        "    fin_outputs = []\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "        text = [data[\"all_tokens\"]  for data in batch]\n",
        "        labels = [data[\"labels\"][label_index] for data in batch]\n",
        "        targets = []\n",
        "        if len(labels) > 1:\n",
        "            for label_set in labels:\n",
        "              miniset = []\n",
        "              for label in label_set:\n",
        "                miniset.append(torch.tensor([label]))\n",
        "              targets.append(torch.stack(miniset))\n",
        "        else:\n",
        "            miniset = [torch.tensor([labels[0]])]\n",
        "            targets.append(torch.stack(miniset))\n",
        "        diagnostics = [data[\"diagnostics\"] for data in batch]\n",
        "\n",
        "        text = torch.cat(text)\n",
        "        text = text.to(device)\n",
        "\n",
        "        # text = torch.cat(text)\n",
        "        targets = torch.cat(targets)\n",
        "        diagnostics = torch.cat(diagnostics)\n",
        "        diagnostics = torch.stack([diagnostics])\n",
        "\n",
        "        # ids = text.to(device, dtype=torch.long)\n",
        "        # mask = mask.to(device, dtype=torch.long)\n",
        "        # token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        targets = targets.to(device, dtype=torch.float)\n",
        "        diagnostics = diagnostics.to(device, dtype=torch.long)\n",
        "\n",
        "        input_ids_diags = torch.cat((text, diagnostics), dim=1)\n",
        "        input_ids_diags = input_ids_diags.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids_diags=input_ids_diags)\n",
        "            \n",
        "        outputs = torch.sigmoid(outputs)\n",
        "        fin_targets.append(targets.cpu().detach().numpy())\n",
        "        fin_outputs.append(outputs.cpu().detach().numpy())\n",
        "    return np.concatenate(fin_outputs), np.concatenate(fin_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwDyxJneo4-0",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "class Single_Flair_Model(nn.Module):\n",
        "    \"\"\" A Model for bert fine tuning \"\"\"\n",
        "\n",
        "    def __init__(self, n_diags):\n",
        "        super(Single_Flair_Model, self).__init__()\n",
        "        self.embedding = TransformerDocumentEmbeddings(\"dccuchile/bert-base-spanish-wwm-cased\", fine_tune=True, layers='-1')\n",
        "        self.out = nn.Linear(768 + n_diags, 1)\n",
        "\n",
        "    def forward(self, sentence_inp, diagnostics):\n",
        "        self.embedding.embed(sentence_inp)\n",
        "        emb = [sentence.get_embedding() for sentence in sentence_inp]\n",
        "        output = torch.stack(emb)\n",
        "        output = torch.cat((output, diagnostics), dim=1)\n",
        "        output = self.out(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9bl5CgWvpZP",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "emb_model = torch.load(\"/research/jamunoz/models/flair_fine_tuning/model_ft_0.pt\", map_location=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQsAPZGL0kIH",
        "jupyter": {
          "outputs_hidden": false
        },
        "scrolled": true,
        "outputId": "e8d9ae25-5801-4499-a6dd-2434cd7ab488"
      },
      "source": [
        "EPOCH = 30\n",
        "LABEL_INDEX = 0\n",
        "GRADIENT_ACCUMULATIONS = 16\n",
        "lr = 1e-5\n",
        "\n",
        "num_training_steps=int(len(transformed_train_dataset) / TRAIN_BATCH_SIZE * EPOCH)\n",
        "\n",
        "model=New_Model(embedding_model=emb_model.embedding.model, n_diags=len(diagnoses_keys)).to(device)\n",
        "optimizer=AdamW(model.parameters(), lr=lr)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                      num_warmup_steps=0,\n",
        "                                      num_training_steps=num_training_steps)\n",
        "val_losses=[]\n",
        "batches_losses=[]\n",
        "val_acc=[]\n",
        "\n",
        "best_model = None\n",
        "best_f1 = 0\n",
        "th = 0\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "  t0 = time.time()    \n",
        "  print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
        "  # Modify according to individual or all\n",
        "  batches_losses_tmp=individual_train_loop_fun1(train_data_loader, model, optimizer, device, GRADIENT_ACCUMULATIONS, scheduler=scheduler, label_index=LABEL_INDEX)\n",
        "  epoch_loss=np.mean(batches_losses_tmp)\n",
        "  print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
        "  t1=time.time()\n",
        "  # Modify according to individual or all\n",
        "  output, target, val_losses_tmp=individual_eval_loop_fun1(val_data_loader, model, device, label_index=LABEL_INDEX)\n",
        "  print(f\"==> evaluation : avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\\n\")\n",
        "  tmp_evaluate=individual_evaluation(target, output)\n",
        "  print(f\"=====>\\t{tmp_evaluate}\")\n",
        "  th = tmp_evaluate[\"threshold\"]\n",
        "  val_acc.append(tmp_evaluate['accuracy'])\n",
        "  val_losses.append(val_losses_tmp)\n",
        "  batches_losses.append(batches_losses_tmp)\n",
        "    \n",
        "torch.save(model, f\"/research/jamunoz/models/flair_fine_tuning/lstm_model_\"+str(LABEL_INDEX)+\"_v2.pt\")\n",
        "# model = torch.load(\"/research/jamunoz/models/flair_fine_tuning/lstm_model_\"+str(LABEL_INDEX)+\".pt\", map_location=device)\n",
        "# th = 0.44041115\n",
        "output, target=individual_test_loop_fun1(test_data_loader, model, device, label_index=LABEL_INDEX)\n",
        "tmp_test=individual_test(target, output, th)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=============== EPOCH 1 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.5615, time = 0.85 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.9095, time = 101.21 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.5609, time = 101.80 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.6130, time = 100.98 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.7059, time = 101.98 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.7225, time = 101.14 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.5673, time = 101.66 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.8253, time = 102.23 seconds ___\n",
            "\n",
            "*** avg_loss : 0.70, time : ~12.0 min (747.08 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.70, time : 64.73 sec\n",
            "\n",
            "threshold:  [0.34935382]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.34935382, 'accuracy': 0.4695222405271829, 'f1': 0.47039473684210525, 'recall': 0.5437262357414449, 'specificity': 0.4127906976744186, 'default_accuracy': 0.5667215815485996, 'default_f1': 0.0, 'default_recall': 0.0, 'default_specificity': 1.0}\n",
            "\n",
            "=============== EPOCH 2 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.4964, time = 0.40 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.9425, time = 101.70 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.6015, time = 101.86 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.6312, time = 101.23 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.7026, time = 102.47 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.7042, time = 101.51 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.6348, time = 101.81 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.6150, time = 102.42 seconds ___\n",
            "\n",
            "*** avg_loss : 0.68, time : ~12.0 min (748.68 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.71, time : 64.69 sec\n",
            "\n",
            "threshold:  [0.31637806]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.31637806, 'accuracy': 0.46787479406919275, 'f1': 0.4696223316912972, 'recall': 0.5437262357414449, 'specificity': 0.40988372093023256, 'default_accuracy': 0.5667215815485996, 'default_f1': 0.0, 'default_recall': 0.0, 'default_specificity': 1.0}\n",
            "\n",
            "=============== EPOCH 3 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.3980, time = 0.40 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.9352, time = 102.45 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.6396, time = 101.94 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.5958, time = 101.91 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.6919, time = 101.38 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.7161, time = 101.29 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.5578, time = 101.57 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.8689, time = 102.59 seconds ___\n",
            "\n",
            "*** avg_loss : 0.70, time : ~12.0 min (748.91 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.68, time : 64.52 sec\n",
            "\n",
            "threshold:  [0.46575782]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.46575782, 'accuracy': 0.5271828665568369, 'f1': 0.528735632183908, 'recall': 0.6121673003802282, 'specificity': 0.4622093023255814, 'default_accuracy': 0.5733113673805601, 'default_f1': 0.05128205128205128, 'default_recall': 0.026615969581749048, 'default_specificity': 0.9912790697674418}\n",
            "\n",
            "=============== EPOCH 4 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.5248, time = 0.40 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.8661, time = 102.30 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.6761, time = 102.23 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.5822, time = 100.57 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.7268, time = 101.15 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.7053, time = 99.97 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.6055, time = 100.21 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.8724, time = 101.22 seconds ___\n",
            "\n",
            "*** avg_loss : 0.70, time : ~12.0 min (742.70 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.69, time : 64.51 sec\n",
            "\n",
            "threshold:  [0.5253354]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.5253354, 'accuracy': 0.5436573311367381, 'f1': 0.5451559934318554, 'recall': 0.6311787072243346, 'specificity': 0.47674418604651164, 'default_accuracy': 0.49917627677100496, 'default_f1': 0.5681818181818182, 'default_recall': 0.7604562737642585, 'default_specificity': 0.29941860465116277}\n",
            "\n",
            "=============== EPOCH 5 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.5197, time = 0.39 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.7635, time = 100.79 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.6506, time = 100.28 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.5869, time = 100.36 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.7246, time = 101.30 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.7036, time = 99.69 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.6521, time = 99.95 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.7975, time = 101.45 seconds ___\n",
            "\n",
            "*** avg_loss : 0.68, time : ~12.0 min (739.01 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.68, time : 65.15 sec\n",
            "\n",
            "threshold:  [0.5073913]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.5073913, 'accuracy': 0.5551894563426688, 'f1': 0.555921052631579, 'recall': 0.6425855513307985, 'specificity': 0.4883720930232558, 'default_accuracy': 0.5321252059308073, 'default_f1': 0.5657492354740061, 'default_recall': 0.7034220532319392, 'default_specificity': 0.4011627906976744}\n",
            "\n",
            "=============== EPOCH 6 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.4459, time = 0.40 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.7206, time = 101.24 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.6397, time = 100.83 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.5833, time = 100.39 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.7153, time = 101.76 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.7057, time = 101.21 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.6365, time = 100.84 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.7522, time = 101.66 seconds ___\n",
            "\n",
            "*** avg_loss : 0.68, time : ~12.0 min (743.34 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.68, time : 65.35 sec\n",
            "\n",
            "threshold:  [0.48727447]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.48727447, 'accuracy': 0.5551894563426688, 'f1': 0.555921052631579, 'recall': 0.6425855513307985, 'specificity': 0.4883720930232558, 'default_accuracy': 0.5584843492586491, 'default_f1': 0.5073529411764706, 'default_recall': 0.5247148288973384, 'default_specificity': 0.5843023255813954}\n",
            "\n",
            "=============== EPOCH 7 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.4298, time = 0.39 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.7091, time = 101.58 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.6072, time = 101.31 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.5793, time = 100.52 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.7213, time = 101.77 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.6636, time = 101.96 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.6394, time = 101.47 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.7418, time = 101.86 seconds ___\n",
            "\n",
            "*** avg_loss : 0.67, time : ~12.0 min (745.95 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.67, time : 64.42 sec\n",
            "\n",
            "threshold:  [0.44259384]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.44259384, 'accuracy': 0.5518945634266886, 'f1': 0.5526315789473685, 'recall': 0.6387832699619772, 'specificity': 0.48546511627906974, 'default_accuracy': 0.6029654036243822, 'default_f1': 0.45842696629213486, 'default_recall': 0.38783269961977185, 'default_specificity': 0.7674418604651163}\n",
            "\n",
            "=============== EPOCH 8 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.4550, time = 0.40 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.7113, time = 101.93 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.6275, time = 102.03 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.5537, time = 101.20 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.7303, time = 102.67 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.6737, time = 101.77 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.5781, time = 102.09 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.6461, time = 102.78 seconds ___\n",
            "\n",
            "*** avg_loss : 0.65, time : ~12.0 min (750.15 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.67, time : 64.73 sec\n",
            "\n",
            "threshold:  [0.38068062]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.38068062, 'accuracy': 0.5535420098846787, 'f1': 0.555008210180624, 'recall': 0.6425855513307985, 'specificity': 0.48546511627906974, 'default_accuracy': 0.6128500823723229, 'default_f1': 0.26332288401253917, 'default_recall': 0.1596958174904943, 'default_specificity': 0.9593023255813954}\n",
            "\n",
            "=============== EPOCH 9 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.2916, time = 0.40 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.7810, time = 102.36 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.6095, time = 101.60 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.5743, time = 99.44 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.7012, time = 100.08 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.6839, time = 99.70 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.6171, time = 99.40 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.6523, time = 100.52 seconds ___\n",
            "\n",
            "*** avg_loss : 0.64, time : ~12.0 min (737.65 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.66, time : 63.16 sec\n",
            "\n",
            "threshold:  [0.3823683]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.3823683, 'accuracy': 0.5634266886326195, 'f1': 0.5648604269293924, 'recall': 0.6539923954372624, 'specificity': 0.4941860465116279, 'default_accuracy': 0.6128500823723229, 'default_f1': 0.4226044226044226, 'default_recall': 0.3269961977186312, 'default_specificity': 0.8313953488372093}\n",
            "\n",
            "=============== EPOCH 10 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.2988, time = 0.39 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.7114, time = 100.41 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.6467, time = 99.91 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.5372, time = 99.46 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.6926, time = 100.09 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.6327, time = 99.25 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.5485, time = 99.22 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.6519, time = 100.17 seconds ___\n",
            "\n",
            "*** avg_loss : 0.62, time : ~12.0 min (733.33 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.66, time : 63.07 sec\n",
            "\n",
            "threshold:  [0.3370759]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.3370759, 'accuracy': 0.5420098846787479, 'f1': 0.5427631578947368, 'recall': 0.6273764258555133, 'specificity': 0.47674418604651164, 'default_accuracy': 0.6079077429983526, 'default_f1': 0.405, 'default_recall': 0.30798479087452474, 'default_specificity': 0.8372093023255814}\n",
            "\n",
            "=============== EPOCH 11 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.2775, time = 0.39 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.7592, time = 100.32 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.6675, time = 99.50 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.5311, time = 98.78 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.6061, time = 99.67 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.5974, time = 99.07 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.5978, time = 98.71 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.6462, time = 99.54 seconds ___\n",
            "\n",
            "*** avg_loss : 0.59, time : ~12.0 min (730.19 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.69, time : 63.06 sec\n",
            "\n",
            "threshold:  [0.2666025]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.2666025, 'accuracy': 0.5584843492586491, 'f1': 0.5592105263157896, 'recall': 0.6463878326996197, 'specificity': 0.49127906976744184, 'default_accuracy': 0.6013179571663921, 'default_f1': 0.4716157205240175, 'default_recall': 0.41064638783269963, 'default_specificity': 0.747093023255814}\n",
            "\n",
            "=============== EPOCH 12 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.2207, time = 0.39 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.8027, time = 99.10 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.6816, time = 99.15 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.5256, time = 98.83 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.5434, time = 100.06 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.4996, time = 98.65 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.6048, time = 98.64 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.4832, time = 99.53 seconds ___\n",
            "\n",
            "*** avg_loss : 0.56, time : ~12.0 min (728.58 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.70, time : 63.27 sec\n",
            "\n",
            "threshold:  [0.25043595]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.25043595, 'accuracy': 0.5518945634266886, 'f1': 0.5526315789473685, 'recall': 0.6387832699619772, 'specificity': 0.48546511627906974, 'default_accuracy': 0.6128500823723229, 'default_f1': 0.4835164835164835, 'default_recall': 0.41825095057034223, 'default_specificity': 0.7616279069767442}\n",
            "\n",
            "=============== EPOCH 13 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.1966, time = 0.39 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.7920, time = 99.56 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.6691, time = 99.19 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.5102, time = 98.21 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.4326, time = 99.54 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.4880, time = 98.70 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.4563, time = 99.00 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.3792, time = 99.73 seconds ___\n",
            "\n",
            "*** avg_loss : 0.54, time : ~12.0 min (729.13 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.71, time : 63.46 sec\n",
            "\n",
            "threshold:  [0.23397721]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.23397721, 'accuracy': 0.5848434925864909, 'f1': 0.5855263157894737, 'recall': 0.6768060836501901, 'specificity': 0.5145348837209303, 'default_accuracy': 0.6112026359143328, 'default_f1': 0.4660633484162896, 'default_recall': 0.3916349809885932, 'default_specificity': 0.7790697674418605}\n",
            "\n",
            "=============== EPOCH 14 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.2460, time = 0.39 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.6815, time = 99.69 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.6726, time = 99.11 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.4664, time = 98.48 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.3909, time = 18521.82 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.3954, time = 97.80 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.4674, time = 97.55 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.2909, time = 98.96 seconds ___\n",
            "\n",
            "*** avg_loss : 0.48, time : ~319.0 min (19147.63 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.75, time : 62.54 sec\n",
            "\n",
            "threshold:  [0.20022792]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.20022792, 'accuracy': 0.5518945634266886, 'f1': 0.5526315789473685, 'recall': 0.6387832699619772, 'specificity': 0.48546511627906974, 'default_accuracy': 0.5963756177924218, 'default_f1': 0.5109780439121756, 'default_recall': 0.4866920152091255, 'default_specificity': 0.6802325581395349}\n",
            "\n",
            "=============== EPOCH 15 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.1306, time = 0.39 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.6806, time = 98.29 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.6456, time = 98.49 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.4017, time = 97.41 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.2189, time = 98.90 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.4398, time = 97.68 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.3419, time = 97.71 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.3127, time = 98.75 seconds ___\n",
            "\n",
            "*** avg_loss : 0.43, time : ~12.0 min (721.68 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.80, time : 62.33 sec\n",
            "\n",
            "threshold:  [0.19951563]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.19951563, 'accuracy': 0.5617792421746294, 'f1': 0.5625000000000001, 'recall': 0.6501901140684411, 'specificity': 0.4941860465116279, 'default_accuracy': 0.5996705107084019, 'default_f1': 0.5244618395303327, 'default_recall': 0.5095057034220533, 'default_specificity': 0.6686046511627907}\n",
            "\n",
            "=============== EPOCH 16 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.1198, time = 0.39 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.5818, time = 98.71 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.5437, time = 98.37 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.3290, time = 98.23 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.1416, time = 99.32 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.3371, time = 98.18 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.2854, time = 97.48 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.1559, time = 98.51 seconds ___\n",
            "\n",
            "*** avg_loss : 0.36, time : ~12.0 min (723.18 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.84, time : 62.41 sec\n",
            "\n",
            "threshold:  [0.322711]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.322711, 'accuracy': 0.5815485996705108, 'f1': 0.5822368421052632, 'recall': 0.6730038022813688, 'specificity': 0.5116279069767442, 'default_accuracy': 0.5815485996705108, 'default_f1': 0.5559440559440559, 'default_recall': 0.6045627376425855, 'default_specificity': 0.563953488372093}\n",
            "\n",
            "=============== EPOCH 17 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.0865, time = 0.38 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.4244, time = 98.31 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.6655, time = 98.35 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.2031, time = 97.35 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.2038, time = 99.11 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.2881, time = 97.86 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.2447, time = 98.17 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.1540, time = 98.94 seconds ___\n",
            "\n",
            "*** avg_loss : 0.33, time : ~12.0 min (722.53 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.90, time : 62.39 sec\n",
            "\n",
            "threshold:  [0.31949753]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.31949753, 'accuracy': 0.5749588138385503, 'f1': 0.5756578947368421, 'recall': 0.6653992395437263, 'specificity': 0.5058139534883721, 'default_accuracy': 0.5881383855024712, 'default_f1': 0.5583038869257951, 'default_recall': 0.6007604562737643, 'default_specificity': 0.5784883720930233}\n",
            "\n",
            "=============== EPOCH 18 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.0678, time = 0.38 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.4836, time = 98.19 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.5182, time = 98.23 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.2407, time = 97.69 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.0819, time = 98.73 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.3212, time = 97.56 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.1608, time = 97.77 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.0531, time = 98.40 seconds ___\n",
            "\n",
            "*** avg_loss : 0.30, time : ~12.0 min (720.91 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.82, time : 62.25 sec\n",
            "\n",
            "threshold:  [0.1823975]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.1823975, 'accuracy': 0.6112026359143328, 'f1': 0.6118421052631579, 'recall': 0.7072243346007605, 'specificity': 0.5377906976744186, 'default_accuracy': 0.6326194398682042, 'default_f1': 0.5686653771760154, 'default_recall': 0.55893536121673, 'default_specificity': 0.688953488372093}\n",
            "\n",
            "=============== EPOCH 19 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.0634, time = 0.38 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.7408, time = 98.46 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.5368, time = 97.85 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.1218, time = 97.43 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.0681, time = 98.67 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.2953, time = 97.59 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.1984, time = 97.74 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.1106, time = 98.61 seconds ___\n",
            "\n",
            "*** avg_loss : 0.27, time : ~12.0 min (720.75 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.89, time : 62.23 sec\n",
            "\n",
            "threshold:  [0.31459615]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.31459615, 'accuracy': 0.5947281713344317, 'f1': 0.5953947368421052, 'recall': 0.688212927756654, 'specificity': 0.5232558139534884, 'default_accuracy': 0.6013179571663921, 'default_f1': 0.56, 'default_recall': 0.5855513307984791, 'default_specificity': 0.6133720930232558}\n",
            "\n",
            "=============== EPOCH 20 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.0616, time = 0.38 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.4857, time = 98.33 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.2965, time = 97.89 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.1216, time = 97.35 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.0488, time = 98.79 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.3021, time = 97.72 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.0997, time = 97.71 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.1023, time = 98.22 seconds ___\n",
            "\n",
            "*** avg_loss : 0.23, time : ~12.0 min (720.69 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.91, time : 63.55 sec\n",
            "\n",
            "threshold:  [0.24559833]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.24559833, 'accuracy': 0.6112026359143328, 'f1': 0.6118421052631579, 'recall': 0.7072243346007605, 'specificity': 0.5377906976744186, 'default_accuracy': 0.6342668863261944, 'default_f1': 0.5873605947955389, 'default_recall': 0.6007604562737643, 'default_specificity': 0.6598837209302325}\n",
            "\n",
            "=============== EPOCH 21 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.0559, time = 0.39 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.1215, time = 99.91 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.2544, time = 99.85 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.1120, time = 99.64 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.0281, time = 100.79 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.3233, time = 100.23 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.0764, time = 99.36 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.1212, time = 97.95 seconds ___\n",
            "\n",
            "*** avg_loss : 0.20, time : ~12.0 min (732.23 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.92, time : 63.79 sec\n",
            "\n",
            "threshold:  [0.45680276]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.45680276, 'accuracy': 0.6112026359143328, 'f1': 0.6118421052631579, 'recall': 0.7072243346007605, 'specificity': 0.5377906976744186, 'default_accuracy': 0.6161449752883031, 'default_f1': 0.6084033613445379, 'default_recall': 0.688212927756654, 'default_specificity': 0.561046511627907}\n",
            "\n",
            "=============== EPOCH 22 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.0508, time = 0.39 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.3419, time = 100.40 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.1984, time = 100.26 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.0840, time = 99.74 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.0286, time = 100.72 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.3707, time = 100.03 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.0883, time = 100.25 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.0946, time = 100.54 seconds ___\n",
            "\n",
            "*** avg_loss : 0.19, time : ~12.0 min (736.80 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.91, time : 64.43 sec\n",
            "\n",
            "threshold:  [0.21511923]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.21511923, 'accuracy': 0.6144975288303131, 'f1': 0.6151315789473685, 'recall': 0.7110266159695817, 'specificity': 0.5406976744186046, 'default_accuracy': 0.6474464579901154, 'default_f1': 0.5900383141762452, 'default_recall': 0.5855513307984791, 'default_specificity': 0.6947674418604651}\n",
            "\n",
            "=============== EPOCH 23 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.0428, time = 0.39 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.0544, time = 100.64 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.1919, time = 100.23 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.0611, time = 99.96 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.0201, time = 100.78 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.3416, time = 99.75 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.0518, time = 100.24 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.0810, time = 100.78 seconds ___\n",
            "\n",
            "*** avg_loss : 0.16, time : ~12.0 min (737.26 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.97, time : 64.51 sec\n",
            "\n",
            "threshold:  [0.12888843]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.12888843, 'accuracy': 0.5980230642504119, 'f1': 0.5986842105263157, 'recall': 0.6920152091254753, 'specificity': 0.5261627906976745, 'default_accuracy': 0.657331136738056, 'default_f1': 0.5806451612903226, 'default_recall': 0.5475285171102662, 'default_specificity': 0.7412790697674418}\n",
            "\n",
            "=============== EPOCH 24 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.0390, time = 0.39 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.0467, time = 100.66 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.1774, time = 100.02 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.0496, time = 99.69 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.0756, time = 100.77 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.3392, time = 99.81 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.0912, time = 100.23 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.0757, time = 100.76 seconds ___\n",
            "\n",
            "*** avg_loss : 0.14, time : ~12.0 min (736.99 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 1.02, time : 64.47 sec\n",
            "\n",
            "threshold:  [0.23159732]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.23159732, 'accuracy': 0.6177924217462932, 'f1': 0.618421052631579, 'recall': 0.714828897338403, 'specificity': 0.5436046511627907, 'default_accuracy': 0.6523887973640856, 'default_f1': 0.6085343228200372, 'default_recall': 0.623574144486692, 'default_specificity': 0.6744186046511628}\n",
            "\n",
            "=============== EPOCH 25 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.0386, time = 0.39 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.0877, time = 100.58 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.0924, time = 100.21 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.0853, time = 99.95 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.0095, time = 101.18 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.3529, time = 100.44 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.0912, time = 100.21 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.1454, time = 101.05 seconds ___\n",
            "\n",
            "*** avg_loss : 0.14, time : ~12.0 min (738.78 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 0.99, time : 65.15 sec\n",
            "\n",
            "threshold:  [0.12128305]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.121283054, 'accuracy': 0.6210873146622735, 'f1': 0.6217105263157895, 'recall': 0.7186311787072244, 'specificity': 0.5465116279069767, 'default_accuracy': 0.6655683690280065, 'default_f1': 0.5831622176591376, 'default_recall': 0.5399239543726235, 'default_specificity': 0.7616279069767442}\n",
            "\n",
            "=============== EPOCH 26 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.0346, time = 0.39 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.0157, time = 100.79 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.0597, time = 100.48 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.0401, time = 100.35 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.0095, time = 101.31 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.3096, time = 100.41 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.0938, time = 100.66 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.0793, time = 101.25 seconds ___\n",
            "\n",
            "*** avg_loss : 0.13, time : ~12.0 min (740.49 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 1.06, time : 65.51 sec\n",
            "\n",
            "threshold:  [0.34212294]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.34212294, 'accuracy': 0.6046128500823723, 'f1': 0.6052631578947367, 'recall': 0.6996197718631179, 'specificity': 0.5319767441860465, 'default_accuracy': 0.6392092257001647, 'default_f1': 0.6039783001808319, 'default_recall': 0.6349809885931559, 'default_specificity': 0.6424418604651163}\n",
            "\n",
            "=============== EPOCH 27 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.0213, time = 0.39 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.0148, time = 101.31 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.1063, time = 101.05 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.0402, time = 100.41 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.0101, time = 101.29 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.3569, time = 100.43 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.0377, time = 100.64 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.0295, time = 101.46 seconds ___\n",
            "\n",
            "*** avg_loss : 0.12, time : ~12.0 min (741.79 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 1.11, time : 65.42 sec\n",
            "\n",
            "threshold:  [0.36952057]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.36952057, 'accuracy': 0.6177924217462932, 'f1': 0.618421052631579, 'recall': 0.714828897338403, 'specificity': 0.5436046511627907, 'default_accuracy': 0.6227347611202636, 'default_f1': 0.6017391304347826, 'default_recall': 0.6577946768060836, 'default_specificity': 0.5959302325581395}\n",
            "\n",
            "=============== EPOCH 28 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.0206, time = 0.39 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.0130, time = 101.18 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.0333, time = 101.10 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.0334, time = 100.46 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.0092, time = 101.42 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.3251, time = 100.40 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.0795, time = 100.62 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.0238, time = 101.45 seconds ___\n",
            "\n",
            "*** avg_loss : 0.11, time : ~12.0 min (741.83 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 1.12, time : 65.34 sec\n",
            "\n",
            "threshold:  [0.13699506]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.13699506, 'accuracy': 0.6144975288303131, 'f1': 0.6151315789473685, 'recall': 0.7110266159695817, 'specificity': 0.5406976744186046, 'default_accuracy': 0.6375617792421746, 'default_f1': 0.5752895752895754, 'default_recall': 0.5665399239543726, 'default_specificity': 0.6918604651162791}\n",
            "\n",
            "=============== EPOCH 29 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.0060, time = 0.39 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.0183, time = 101.08 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.0805, time = 100.87 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.0306, time = 100.38 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.0081, time = 101.31 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.3625, time = 100.43 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.0417, time = 100.43 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.0370, time = 101.12 seconds ___\n",
            "\n",
            "*** avg_loss : 0.12, time : ~12.0 min (740.56 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 1.14, time : 65.09 sec\n",
            "\n",
            "threshold:  [0.30012047]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.30012047, 'accuracy': 0.6144975288303131, 'f1': 0.6151315789473685, 'recall': 0.7110266159695817, 'specificity': 0.5406976744186046, 'default_accuracy': 0.6128500823723229, 'default_f1': 0.573502722323049, 'default_recall': 0.6007604562737643, 'default_specificity': 0.622093023255814}\n",
            "\n",
            "=============== EPOCH 30 / 30 ===============\n",
            "\n",
            "___ batch index = 0 / 1836 (0.00%), loss = 0.0092, time = 0.39 seconds ___\n",
            "___ batch index = 250 / 1836 (13.62%), loss = 0.0078, time = 100.85 seconds ___\n",
            "___ batch index = 500 / 1836 (27.23%), loss = 0.0341, time = 100.46 seconds ___\n",
            "___ batch index = 750 / 1836 (40.85%), loss = 0.0439, time = 100.33 seconds ___\n",
            "___ batch index = 1000 / 1836 (54.47%), loss = 0.0160, time = 101.13 seconds ___\n",
            "___ batch index = 1250 / 1836 (68.08%), loss = 0.4056, time = 99.80 seconds ___\n",
            "___ batch index = 1500 / 1836 (81.70%), loss = 0.0274, time = 100.46 seconds ___\n",
            "___ batch index = 1750 / 1836 (95.32%), loss = 0.0331, time = 100.86 seconds ___\n",
            "\n",
            "*** avg_loss : 0.11, time : ~12.0 min (738.86 sec) ***\n",
            "\n",
            "==> evaluation : avg_loss = 1.20, time : 64.84 sec\n",
            "\n",
            "threshold:  [0.3782773]\n",
            "=====>\t{'Positive Rate': array([0.4332784], dtype=float32), 'threshold': 0.3782773, 'accuracy': 0.6079077429983526, 'f1': 0.6085526315789473, 'recall': 0.7034220532319392, 'specificity': 0.5348837209302325, 'default_accuracy': 0.6013179571663921, 'default_f1': 0.578397212543554, 'default_recall': 0.6311787072243346, 'default_specificity': 0.5784883720930233}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "E5hdz-cwNXkg",
        "outputId": "0a52c46d-be04-4ab9-e7bb-0dac56d67047"
      },
      "source": [
        "print(tmp_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Positive Rate': array([0.44425675], dtype=float32), 'accuracy': 0.6452702702702703, 'f1': 0.6534653465346536, 'recall': 0.752851711026616, 'specificity': 0.5592705167173252, 'default_accuracy': 0.6452702702702703, 'default_f1': 0.6379310344827587, 'default_recall': 0.7034220532319392, 'default_specificity': 0.5987841945288754}\n"
          ]
        }
      ]
    }
  ]
}