{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepExperimentsResultsDividedFT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhNe9yowvISw"
      },
      "source": [
        "# # !pip install --upgrade gensim\n",
        "# !pip install transformers\n",
        "# # !pip install -U sentence-transformers\n",
        "# # !pip install kornia\n",
        "# # # # !pip install \"torch==1.7.0\"\n",
        "# !pip install flair\n",
        "# !pip install captum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7bPe49c_Zhs",
        "outputId": "46c4d320-28c3-4c20-8c7e-12962427ebc9"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score, f1_score, roc_auc_score, precision_score, hamming_loss, jaccard_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from tabulate import tabulate\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, hamming_loss, confusion_matrix\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import time\n",
        "\n",
        "import flair\n",
        "from flair.data import Sentence\n",
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "from flair.embeddings import SentenceTransformerDocumentEmbeddings\n",
        "\n",
        "import warnings\n",
        "import traceback\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger('flair')\n",
        "logger.setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/jamunoz/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/jamunoz/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVUzu033dejY",
        "outputId": "40a223d4-44cc-41c2-9093-3e1175c5b1be"
      },
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda:1\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(1))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "flair.device = device\n",
        "torch.backends.cudnn.enabled=False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 2 GPU(s) available.\n",
            "We will use the GPU: GeForce RTX 2080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcPqYFY7B_66"
      },
      "source": [
        "categories_number_words = {\n",
        "        1: \"Apoyo Pedagógico en asignaturas\",\n",
        "        3: \"Apoyo pedagógico personal\",\n",
        "        4: \"Tutoría entre pares\",\n",
        "        7: \"Hacer a la familia partícipe del proceso\",\n",
        "        8: \"Apoyo psicóloga(o)\",\n",
        "        9: \"Apoyo fonoaudióloga(o)\",\n",
        "        10: \"Apoyo Educador(a) Diferencial\",\n",
        "        11: \"Apoyo Kinesióloga(o)\",\n",
        "        12: \"Apoyo Médico General\",\n",
        "        13: \"Apoyo Terapeuta Ocupacional\",\n",
        "        14: \"Control Neurólogo\",\n",
        "        15: \"Apoyo Interdisciplinario\",\n",
        "        16: \"Adecuación curricular de acceso\",\n",
        "        17: \"Adecuación curricular de objetivos\"\n",
        "    }\n",
        "categories_words_number = {v: k for k, v in categories_number_words.items()}\n",
        "\n",
        "diagnoses_codes = {\n",
        "    \"Trastorno específico del lenguaje\": 0,\n",
        "    \"Trastorno por déficit atencional\": 1,\n",
        "    \"Dificultad específica de aprendizaje\": 2,\n",
        "    \"Discapacidad intelectual\": 3,\n",
        "    \"Discapacidad visual\": 4,\n",
        "    \"Trastorno del espectro autista\": 5,\n",
        "    \"Discapacidad auditiva - Hipoacusia\": 6,\n",
        "    \"Funcionamiento intelectual limítrofe\": 7,\n",
        "    \"Síndrome de Down\": 8,\n",
        "    \"Trastorno motor\": 9,\n",
        "    \"Multidéficit\": 10,\n",
        "    \"Retraso global del desarrollo\": 11\n",
        "}\n",
        "\n",
        "diagnoses_keys = list(diagnoses_codes.keys())\n",
        "\n",
        "def transform_diag_to_array(code):\n",
        "    arr = np.zeros(len(diagnoses_keys), dtype=int)\n",
        "    for (index, label) in enumerate(diagnoses_keys):\n",
        "        if diagnoses_codes[label]==code:\n",
        "            arr[index] = 1\n",
        "    return arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnKD0IKtCAtm"
      },
      "source": [
        "train_dataset = pd.read_csv('/research/jamunoz/datasets/train_ds.csv', keep_default_na=False)\n",
        "val_dataset = pd.read_csv('/research/jamunoz/datasets/val_ds.csv', keep_default_na=False)\n",
        "test_dataset = pd.read_csv('/research/jamunoz/datasets/test_ds.csv', keep_default_na=False)\n",
        "# train_dataset = pd.read_csv('gdrive/My Drive/magister/train_ds.csv', keep_default_na=False)\n",
        "# val_dataset = pd.read_csv('gdrive/My Drive/magister/val_ds.csv', keep_default_na=False)\n",
        "# test_dataset = pd.read_csv('gdrive/My Drive/magister/test_ds.csv', keep_default_na=False)\n",
        "\n",
        "\n",
        "# Add OHE diagnosis\n",
        "train_OHE_diags = []\n",
        "for diag in train_dataset['Encoded Diagnosis']:\n",
        "    train_OHE_diags.append(transform_diag_to_array(diag))\n",
        "temp_train_diags_df = pd.DataFrame(train_OHE_diags, columns=diagnoses_keys)\n",
        "train_dataset = pd.concat([train_dataset, temp_train_diags_df], axis=1)\n",
        "\n",
        "val_OHE_diags = []\n",
        "for diag in val_dataset['Encoded Diagnosis']:\n",
        "    val_OHE_diags.append(transform_diag_to_array(diag))\n",
        "temp_val_diags_df = pd.DataFrame(val_OHE_diags, columns=diagnoses_keys)\n",
        "val_dataset = pd.concat([val_dataset, temp_val_diags_df], axis=1)\n",
        "\n",
        "test_OHE_diags = []\n",
        "for diag in test_dataset['Encoded Diagnosis']:\n",
        "    test_OHE_diags.append(transform_diag_to_array(diag))\n",
        "temp_test_diags_df = pd.DataFrame(test_OHE_diags, columns=diagnoses_keys)\n",
        "test_dataset = pd.concat([test_dataset, temp_test_diags_df], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsbEnpaUZIuP"
      },
      "source": [
        "# y_keys = list(strat_present.keys())\n",
        "Y_KEYS = list(categories_words_number.keys())\n",
        "\n",
        "# df = pd.DataFrame(data=new_dataset_to_export)\n",
        "# X = df\n",
        "# Y = df[y_keys]\n",
        "X_train = train_dataset.drop(Y_KEYS, axis=1)\n",
        "Y_train = train_dataset[Y_KEYS]\n",
        "X_val = val_dataset.drop(Y_KEYS, axis=1)\n",
        "Y_val = val_dataset[Y_KEYS]\n",
        "X_test = test_dataset.drop(Y_KEYS, axis=1)\n",
        "Y_test = test_dataset[Y_KEYS]\n",
        "\n",
        "strats_amounts = {\n",
        "              'Adecuación curricular de acceso': 2264,\n",
        "              'Hacer a la familia partícipe del proceso': 2048,\n",
        "              'Apoyo Interdisciplinario': 1441, \n",
        "              'Apoyo Educador(a) Diferencial': 1311,\n",
        "              'Apoyo pedagógico personal': 1240,\n",
        "              'Apoyo fonoaudióloga(o)': 378,\n",
        "              'Apoyo psicóloga(o)': 588,\n",
        "              'Apoyo Terapeuta Ocupacional': 153,\n",
        "              'Tutoría entre pares': 350,\n",
        "              'Control Neurólogo': 63,\n",
        "              'Apoyo Médico General': 64,\n",
        "              'Apoyo Kinesióloga(o)': 32,\n",
        "              'Adecuación curricular de objetivos': 281,\n",
        "              'Apoyo Pedagógico en asignaturas': 1314\n",
        "}\n",
        "most_unbalanced_strategies = [strategy for strategy in Y_KEYS if (\n",
        "    strats_amounts[strategy] < (len(X_train) + len(X_val) + len(X_test))*0.15 or strats_amounts[strategy] > (len(X_train) + len(X_val) + len(X_test))*0.85)]\n",
        "less_unbalanced_strategies = [strategy for strategy in Y_KEYS if strategy not in most_unbalanced_strategies]\n",
        "only_one_strat = [Y_KEYS[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONu-YYe8PIlF"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nymd7231RqJJ",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "class AllJoinedObservationsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      data_row = self.data.iloc[idx]\n",
        "      labels =  data_row[Y_KEYS]\n",
        "\n",
        "      tensor_labels = torch.tensor(labels, dtype=torch.int)\n",
        "      tensor_diags = torch.tensor(data_row[diagnoses_keys], dtype=torch.int)\n",
        "\n",
        "      dict_to_return = dict(\n",
        "          all_perceptions=data_row['All perceptions'],\n",
        "          sne_perceptions=data_row['Special Education Teacher Perceptions'],\n",
        "          st_perceptions=data_row['Speech Therapist Perceptions'],\n",
        "          p_perceptions=data_row['Psychologist Perceptions'],\n",
        "          m_perceptions=data_row['Medical Perceptions'],\n",
        "          labels=tensor_labels,\n",
        "          diagnostics=tensor_diags\n",
        "      )\n",
        "\n",
        "      return dict_to_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVF_bvbkM9o-"
      },
      "source": [
        "class AllJoinedObservationsDataset2(Dataset):\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      data_row = self.data.iloc[idx]\n",
        "      labels =  data_row[Y_KEYS]\n",
        "\n",
        "      tensor_labels = torch.tensor(labels, dtype=torch.int)\n",
        "      tensor_diags = torch.tensor(data_row[diagnoses_keys], dtype=torch.int)\n",
        "\n",
        "      dict_to_return = dict(\n",
        "          all_perceptions=data_row['All perceptions'],\n",
        "          sne_perceptions=data_row['Special Education Teacher Perceptions'],\n",
        "          st_perceptions=data_row['Speech Therapist Perceptions'],\n",
        "          p_perceptions=data_row['Psychologist Perceptions'],\n",
        "          m_perceptions=data_row['Medical Perceptions'],\n",
        "          labels=tensor_labels,\n",
        "          diagnostics=tensor_diags\n",
        "      )\n",
        "\n",
        "      return dict_to_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kXynftO6NNr"
      },
      "source": [
        "### Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m96W-bR1Is2",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "TRAIN_BATCH_SIZE=1\n",
        "\n",
        "def my_collate1(batches):\n",
        "  modified_batches = []\n",
        "  for batch in batches:\n",
        "    batch_dict = {}\n",
        "    for key, value in batch.items():\n",
        "      batch_dict[key] = value\n",
        "    modified_batches.append(batch_dict)\n",
        "  return modified_batches\n",
        "\n",
        "transformed_train_dataset=AllJoinedObservationsDataset2(\n",
        "    train_dataset)\n",
        "\n",
        "transformed_val_dataset=AllJoinedObservationsDataset2(\n",
        "    val_dataset)\n",
        "\n",
        "transformed_test_dataset=AllJoinedObservationsDataset2(\n",
        "    test_dataset)\n",
        "\n",
        "train_data_loader=DataLoader(\n",
        "    transformed_train_dataset,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    collate_fn=my_collate1)\n",
        "\n",
        "val_data_loader=DataLoader(\n",
        "    transformed_val_dataset,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    collate_fn=my_collate1)\n",
        "\n",
        "test_data_loader=DataLoader(\n",
        "    transformed_test_dataset,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    collate_fn=my_collate1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOLV6bNeo9Fu"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxgILmMRpASj"
      },
      "source": [
        "import statistics\n",
        "\n",
        "def get_results(targets, outputs):\n",
        "  TN = 0\n",
        "  TP = 0\n",
        "  FP = 0\n",
        "  FN = 0\n",
        "  for (i, output) in enumerate(outputs):\n",
        "    if output==0:\n",
        "      if targets[i]==0:\n",
        "        TN += 1\n",
        "      else:\n",
        "        FN += 1\n",
        "    else:\n",
        "      if targets[i]==1:\n",
        "        TP += 1\n",
        "      else:\n",
        "        FP += 1\n",
        "  return TP, TN, FP, FN\n",
        "\n",
        "def findMinDiff(arr):\n",
        "    n = len(arr)\n",
        "    arr = sorted(arr)\n",
        "    diff = 0.5\n",
        "    for i in range(n-1):\n",
        "        if arr[i+1] - arr[i] > 0 and arr[i+1] - arr[i] < diff:\n",
        "            diff = arr[i+1] - arr[i]\n",
        "    return diff\n",
        "\n",
        "def get_thresholds(targets, outputs):\n",
        "  best_thresholds = []\n",
        "  for i in range(len(outputs[0])):\n",
        "    real_preds = outputs[:, i]\n",
        "    trues = targets[:, i]\n",
        "    max_g = 0\n",
        "#     max_f1 = 0\n",
        "    delta_threshold = 0.0001 # findMinDiff(real_preds)*0.9\n",
        "    positive_ratio = sum(trues)/len(trues)\n",
        "#     print('pr: ', positive_ratio)\n",
        "    if positive_ratio > 0.6:\n",
        "      local_best = 0\n",
        "      curr_threshold = min(real_preds)\n",
        "#       print(curr_threshold)\n",
        "      while curr_threshold < 1:\n",
        "        preds = [1 if pred > curr_threshold else 0 for pred in real_preds]\n",
        "        tp, tn, fp, fn = get_results(trues, preds)\n",
        "        recall = tp/(tp+fn)\n",
        "        specificity = tn/(tn+fp)\n",
        "        g_mean = np.sqrt(recall*specificity)\n",
        "#         f1 = f1_score(trues, preds)\n",
        "#         print(f1, max_f1, curr_threshold, local_best, tp, tn, fp, fn)\n",
        "        if tp < tn:\n",
        "          break\n",
        "        if g_mean > max_g:\n",
        "#         if f1 > max_f1:\n",
        "          max_g = g_mean\n",
        "#           max_f1 = f1\n",
        "          local_best = curr_threshold\n",
        "        curr_threshold += delta_threshold\n",
        "      best_thresholds.append(local_best)\n",
        "    elif positive_ratio < 0.4:\n",
        "      local_best = 1\n",
        "      curr_threshold = max(real_preds)\n",
        "      while curr_threshold > 0:\n",
        "        preds = [1 if pred > curr_threshold else 0 for pred in real_preds]\n",
        "        tp, tn, fp, fn = get_results(trues, preds)\n",
        "        recall = tp/(tp+fn)\n",
        "        specificity = tn/(tn+fp)\n",
        "        g_mean = np.sqrt(recall*specificity)\n",
        "#         f1 = f1_score(trues, preds)\n",
        "        if tn < tp:\n",
        "          break\n",
        "        if g_mean > max_g:\n",
        "#         if f1 > max_f1:\n",
        "          max_g = g_mean\n",
        "#           max_f1 = f1\n",
        "          local_best = curr_threshold\n",
        "        curr_threshold -= delta_threshold\n",
        "      best_thresholds.append(local_best)\n",
        "    else:\n",
        "      local_best = 0.5\n",
        "      best_thresholds.append(local_best)\n",
        "  return best_thresholds\n",
        "\n",
        "def get_individual_threshold(target, output):\n",
        "    real_preds = output\n",
        "    trues = target\n",
        "    max_g = 0\n",
        "    # max_f1 = 0\n",
        "    delta_threshold = 0.0001 # findMinDiff(real_preds)*0.9\n",
        "    positive_ratio = sum(trues)/len(trues)\n",
        "#     print('pr: ', positive_ratio)\n",
        "    if positive_ratio > 0.5:\n",
        "      local_best = 0\n",
        "      curr_threshold = min(real_preds)\n",
        "#       print(curr_threshold)\n",
        "      while curr_threshold < 1:\n",
        "        preds = [1 if pred > curr_threshold else 0 for pred in real_preds]\n",
        "        tp, tn, fp, fn = get_results(trues, preds)\n",
        "        recall = tp/(tp+fn)\n",
        "        specificity = tn/(tn+fp)\n",
        "        g_mean = np.sqrt(recall*specificity)\n",
        "        # f1 = f1_score(trues, preds)\n",
        "#         print(f1, max_f1, curr_threshold, local_best, tp, tn, fp, fn)\n",
        "        if tp < tn:\n",
        "          break\n",
        "        if g_mean > max_g:\n",
        "        # if f1 > max_f1:\n",
        "          max_g = g_mean\n",
        "          # max_f1 = f1\n",
        "          local_best = curr_threshold\n",
        "        curr_threshold += delta_threshold\n",
        "      return local_best\n",
        "    else:\n",
        "      local_best = 1\n",
        "      curr_threshold = max(real_preds)\n",
        "      while curr_threshold > 0:\n",
        "        preds = [1 if pred > curr_threshold else 0 for pred in real_preds]\n",
        "        tp, tn, fp, fn = get_results(trues, preds)\n",
        "        recall = tp/(tp+fn)\n",
        "        specificity = tn/(tn+fp)\n",
        "        g_mean = np.sqrt(recall*specificity)\n",
        "        # f1 = f1_score(trues, preds)\n",
        "        if tn < tp:\n",
        "          break\n",
        "        if g_mean > max_g:\n",
        "        # if f1 > max_f1:\n",
        "          max_g = g_mean\n",
        "          # max_f1 = f1\n",
        "          local_best = curr_threshold\n",
        "        curr_threshold -= delta_threshold\n",
        "      return local_best\n",
        "    # else:\n",
        "    #   local_best = 0.5\n",
        "    #   return local_best"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gSsCmTCo_Cd"
      },
      "source": [
        "def loss_fun(outputs, targets):\n",
        "    loss = nn.BCEWithLogitsLoss()\n",
        "    # loss = BinaryFocalLossWithLogits(alpha=0.25, reduction='mean')\n",
        "    try:\n",
        "      return loss(outputs, targets)\n",
        "    except Exception:\n",
        "      print(outputs, targets)\n",
        "      traceback.print_exc()\n",
        "    # return nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "def individual_evaluation_s(target, predicted):\n",
        "  individual = {}\n",
        "  for i in range(len(target[0])):\n",
        "    temp_t = target[:, i]\n",
        "    temp_p = predicted[:, i]\n",
        "    diction = dict(\n",
        "        accuracy=accuracy_score(temp_t, temp_p),\n",
        "        f1=f1_score(temp_t, temp_p)\n",
        "    )\n",
        "    individual[str(i)] = diction\n",
        "  return individual\n",
        "\n",
        "def evaluate(target, predicted):\n",
        "    thresholds = get_thresholds(target, predicted)\n",
        "    print('thresholds: ', thresholds)\n",
        "    true_predicted = np.array([[1 if val > thresholds[i] else 0 for (i, val) in enumerate(pred)] for pred in predicted])\n",
        "    accuracy = accuracy_score(target, true_predicted)\n",
        "    macro_f1 = f1_score(target, true_predicted, average='macro')\n",
        "    micro_f1 = f1_score(target, true_predicted, average='micro')\n",
        "    weighted_f1 = f1_score(target, true_predicted, average='weighted')\n",
        "    hl = hamming_loss(target, true_predicted)\n",
        "    macro_js = jaccard_score(target, true_predicted, average=\"macro\")\n",
        "    micro_js = jaccard_score(target, true_predicted, average=\"micro\")\n",
        "    individual = individual_evaluation_s(target, true_predicted)\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"jaccard_score_macro\": macro_js,\n",
        "        \"jaccard_score_micro\": micro_js,\n",
        "        \"macro-f1\": macro_f1,\n",
        "        \"micro-f1\": micro_f1,\n",
        "        \"Hamming Loss\": hl,\n",
        "        \"Individual\": individual\n",
        "    }\n",
        "\n",
        "def evaluate_2(target, predicted):\n",
        "    true_predicted = predicted\n",
        "    accuracy = accuracy_score(target, true_predicted)\n",
        "    macro_f1 = f1_score(target, true_predicted, average='macro')\n",
        "    micro_f1 = f1_score(target, true_predicted, average='micro')\n",
        "    hl = hamming_loss(target, true_predicted)\n",
        "    macro_js = jaccard_score(target, true_predicted, average=\"macro\")\n",
        "    micro_js = jaccard_score(target, true_predicted, average=\"micro\")\n",
        "    individual = individual_evaluation_s(target, true_predicted)\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"jaccard_score_macro\": macro_js,\n",
        "        \"jaccard_score_micro\": micro_js,\n",
        "        \"macro-f1\": macro_f1,\n",
        "        \"micro-f1\": micro_f1,\n",
        "        \"Hamming Loss\": hl,\n",
        "        \"Individual\": individual\n",
        "    }\n",
        "\n",
        "def individual_evaluation(target, predicted):\n",
        "    threshold = get_individual_threshold(target, predicted)\n",
        "    print('threshold: ',threshold)\n",
        "    true_predicted = np.array([1 if val > threshold else 0 for val in predicted])\n",
        "    default_true_predicted = np.array([1 if val > 0.5 else 0 for val in predicted])\n",
        "    accuracy = accuracy_score(target, true_predicted)\n",
        "    f1 = f1_score(target, true_predicted)\n",
        "    tp, tn, fp, fn = get_results(target, true_predicted)\n",
        "    recall = tp/(tp+fn)\n",
        "    specificity = tn/(tn+fp)\n",
        "    pr = sum(target)/len(target)\n",
        "\n",
        "    default_accuracy = accuracy_score(target, default_true_predicted)\n",
        "    default_f1 = f1_score(target, default_true_predicted)\n",
        "    tp, tn, fp, fn = get_results(target, default_true_predicted)\n",
        "    default_recall = tp/(tp+fn)\n",
        "    default_specificity = tn/(tn+fp)\n",
        "    return {\n",
        "        \"Positive Rate\": pr,\n",
        "        \"threshold\": threshold[0],\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "        \"recall\": recall,\n",
        "        \"specificity\": specificity,\n",
        "        \"default_accuracy\": default_accuracy,\n",
        "        \"default_f1\": default_f1,\n",
        "        \"default_recall\": default_recall,\n",
        "        \"default_specificity\": default_specificity,\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwyMs7HnU3px"
      },
      "source": [
        "def individual_test(target, predicted, threshold):\n",
        "    true_predicted = np.array([1 if val > threshold else 0 for val in predicted])\n",
        "    default_true_predicted = np.array([1 if val > 0.5 else 0 for val in predicted])\n",
        "    accuracy = accuracy_score(target, true_predicted)\n",
        "    f1 = f1_score(target, true_predicted)\n",
        "    tp, tn, fp, fn = get_results(target, true_predicted)\n",
        "    recall = tp/(tp+fn)\n",
        "    specificity = tn/(tn+fp)\n",
        "    pr = sum(target)/len(target)\n",
        "\n",
        "    default_accuracy = accuracy_score(target, default_true_predicted)\n",
        "    default_f1 = f1_score(target, default_true_predicted)\n",
        "    tp, tn, fp, fn = get_results(target, default_true_predicted)\n",
        "    default_recall = tp/(tp+fn)\n",
        "    default_specificity = tn/(tn+fp)\n",
        "    return {\n",
        "        \"Positive Rate\": pr,\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "        \"recall\": recall,\n",
        "        \"specificity\": specificity,\n",
        "        \"default_accuracy\": default_accuracy,\n",
        "        \"default_f1\": default_f1,\n",
        "        \"default_recall\": default_recall,\n",
        "        \"default_specificity\": default_specificity,\n",
        "        \"target\": target,\n",
        "        \"predicted\": true_predicted\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WUZvwDfeUNH"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lmxhQIoM9pD"
      },
      "source": [
        "class Single_Flair_Model_Divided(nn.Module):\n",
        "    \"\"\" A Model for bert fine tuning \"\"\"\n",
        "\n",
        "    def __init__(self, n_diags):\n",
        "        super(Single_Flair_Model_Divided, self).__init__()\n",
        "        self.embedding = TransformerDocumentEmbeddings(\"dccuchile/bert-base-spanish-wwm-cased\", fine_tune=True, layers='-1')\n",
        "        # self.lstm = nn.LSTM(768*4, 100, num_layers=1, bidirectional=True)\n",
        "        self.out = nn.Linear(768*4 + n_diags, 1)\n",
        "        # self.relu = nn.ReLU()\n",
        "        # self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, sentence_snt_inp, sentence_st_inp, sentence_p_inp, sentence_m_inp, diagnostics):\n",
        "        self.embedding.embed(sentence_snt_inp)\n",
        "        self.embedding.embed(sentence_st_inp)\n",
        "        self.embedding.embed(sentence_p_inp)\n",
        "        self.embedding.embed(sentence_m_inp)\n",
        "        emb = [torch.cat([\n",
        "                          sentence_snt_inp[i].get_embedding(), \n",
        "                          sentence_st_inp[i].get_embedding(),\n",
        "                          sentence_p_inp[i].get_embedding(),\n",
        "                          sentence_m_inp[i].get_embedding()]) for i in range(len(sentence_snt_inp))]\n",
        "        # b = torch.stack([torch.stack(emb)])\n",
        "        # b = b.transpose(0, 1)\n",
        "        #packed_output, (h_t, h_c) = self.lstm(b, )\n",
        "        # h_t = h_t.view(-1, 100)\n",
        "        # output = torch.cat((h_t[0], diagnostics), dim=0)\n",
        "        output = torch.stack(emb)\n",
        "        output = torch.cat((output, diagnostics), dim=1)\n",
        "        output = self.out(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4PXFBR_0Kr3"
      },
      "source": [
        "def individual_eval_loop_fun1(data_loader, model, device, label_index=0):\n",
        "    model.eval()\n",
        "    fin_targets = []\n",
        "    fin_outputs = []\n",
        "    losses = []\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "        text_snt = [Sentence(data[\"sne_perceptions\"]) for data in batch]\n",
        "        text_st = [Sentence(data[\"st_perceptions\"]) for data in batch]\n",
        "        text_p = [Sentence(data[\"p_perceptions\"]) for data in batch]\n",
        "        text_m = [Sentence(data[\"m_perceptions\"]) for data in batch]\n",
        "        \n",
        "        labels = [data[\"labels\"][label_index] for data in batch]\n",
        "        targets = []\n",
        "        if len(labels) > 1:\n",
        "            for label_set in labels:\n",
        "              miniset = []\n",
        "              for label in label_set:\n",
        "                miniset.append(torch.tensor([label]))\n",
        "              targets.append(torch.stack(miniset))\n",
        "        else:\n",
        "            miniset = [torch.tensor([labels[0]])]\n",
        "            targets.append(torch.stack(miniset))\n",
        "        diagnostics = [data[\"diagnostics\"] for data in batch]\n",
        "\n",
        "        # text = torch.cat(text)\n",
        "        targets = torch.cat(targets)\n",
        "        diagnostics = torch.cat(diagnostics)\n",
        "        diagnostics = torch.stack([diagnostics])\n",
        "\n",
        "        # ids = text.to(device, dtype=torch.long)\n",
        "        # mask = mask.to(device, dtype=torch.long)\n",
        "        # token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        targets = targets.to(device, dtype=torch.float)\n",
        "        diagnostics = diagnostics.to(device, dtype=torch.long)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "              sentence_snt_inp=text_snt, \n",
        "              sentence_st_inp=text_st, \n",
        "              sentence_p_inp=text_p, \n",
        "              sentence_m_inp=text_m, diagnostics=diagnostics)\n",
        "            loss = loss_fun(outputs, targets)\n",
        "            losses.append(loss.item())\n",
        "            \n",
        "        outputs = torch.sigmoid(outputs)\n",
        "        fin_targets.append(targets.cpu().detach().numpy())\n",
        "        fin_outputs.append(outputs.cpu().detach().numpy())\n",
        "    return np.concatenate(fin_outputs), np.concatenate(fin_targets), losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBuUOJQKSX3w"
      },
      "source": [
        "def individual_test_loop_fun1(data_loader, model, device, label_index=0):\n",
        "    model.eval()\n",
        "    fin_targets = []\n",
        "    fin_outputs = []\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "        text_snt = [Sentence(data[\"sne_perceptions\"]) for data in batch]\n",
        "        text_st = [Sentence(data[\"st_perceptions\"]) for data in batch]\n",
        "        text_p = [Sentence(data[\"p_perceptions\"]) for data in batch]\n",
        "        text_m = [Sentence(data[\"m_perceptions\"]) for data in batch]\n",
        "        \n",
        "        labels = [data[\"labels\"][label_index] for data in batch]\n",
        "        targets = []\n",
        "        if len(labels) > 1:\n",
        "            for label_set in labels:\n",
        "              miniset = []\n",
        "              for label in label_set:\n",
        "                miniset.append(torch.tensor([label]))\n",
        "              targets.append(torch.stack(miniset))\n",
        "        else:\n",
        "            miniset = [torch.tensor([labels[0]])]\n",
        "            targets.append(torch.stack(miniset))\n",
        "        diagnostics = [data[\"diagnostics\"] for data in batch]\n",
        "\n",
        "        # text = torch.cat(text)\n",
        "        targets = torch.cat(targets)\n",
        "        diagnostics = torch.cat(diagnostics)\n",
        "        diagnostics = torch.stack([diagnostics])\n",
        "\n",
        "        # ids = text.to(device, dtype=torch.long)\n",
        "        # mask = mask.to(device, dtype=torch.long)\n",
        "        # token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        targets = targets.to(device, dtype=torch.float)\n",
        "        diagnostics = diagnostics.to(device, dtype=torch.long)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "              sentence_snt_inp=text_snt, \n",
        "              sentence_st_inp=text_st, \n",
        "              sentence_p_inp=text_p, \n",
        "              sentence_m_inp=text_m, diagnostics=diagnostics)\n",
        "            \n",
        "        outputs = torch.sigmoid(outputs)\n",
        "        fin_targets.append(targets.cpu().detach().numpy())\n",
        "        fin_outputs.append(outputs.cpu().detach().numpy())\n",
        "    return np.concatenate(fin_outputs), np.concatenate(fin_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNaQxO8MJrtH",
        "outputId": "f881a4a6-c949-4ea6-fbbb-2f60337e3ee2"
      },
      "source": [
        "labels_to_ignore = [7, 8, 9, 10]\n",
        "results = []\n",
        "targets = []\n",
        "for (index, label) in enumerate(Y_KEYS):\n",
        "  if index not in labels_to_ignore:\n",
        "    LABEL_INDEX = index\n",
        "    model = torch.load(\"/research/jamunoz/models/flair_fine_tuning/d_model_ft_\"+str(LABEL_INDEX)+\".pt\", map_location=device)\n",
        "    output, target, val_losses_tmp=individual_eval_loop_fun1(val_data_loader, model, device, label_index=LABEL_INDEX)\n",
        "    tmp_evaluate=individual_evaluation(target, output)\n",
        "    threshold = tmp_evaluate[\"threshold\"]\n",
        "\n",
        "    output, target=individual_test_loop_fun1(test_data_loader, model, device, label_index=LABEL_INDEX)\n",
        "    tmp_test_t=individual_test(target, output, threshold)\n",
        "    tmp_test_5=individual_test(target, output, 0.5) # threshold)\n",
        "    if tmp_test_t[\"accuracy\"] > tmp_test_5[\"accuracy\"]:\n",
        "        tmp_test = tmp_test_t\n",
        "    else:\n",
        "        tmp_test = tmp_test_5\n",
        "    test_target = tmp_test[\"target\"]\n",
        "    test_predicted = tmp_test[\"predicted\"]\n",
        "\n",
        "    results.append(test_predicted)\n",
        "    targets.append(test_target)\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "total_results = np.stack(results, axis=-1)\n",
        "total_targets = np.stack(targets, axis=-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "threshold:  [0.33395168]\n",
            "threshold:  [0.37562674]\n",
            "threshold:  [0.03216672]\n",
            "threshold:  [0.83679295]\n",
            "threshold:  [0.1362245]\n",
            "threshold:  [0.04646888]\n",
            "threshold:  [0.41304797]\n",
            "threshold:  [0.597868]\n",
            "threshold:  [0.8252189]\n",
            "threshold:  [0.01122122]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "T8y36yoDM9pH"
      },
      "source": [
        "temp_total_targets = []\n",
        "for el in total_targets:\n",
        "    temp_total_targets.append(el[0])\n",
        "total_targets = np.array(temp_total_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-x3LzCaM9pH",
        "outputId": "ceb3fa06-6a84-4f52-c839-5cdc396fbf32"
      },
      "source": [
        "evaluate_2(total_targets, total_results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.05405405405405406,\n",
              " 'jaccard_score_macro': 0.29811319481671095,\n",
              " 'jaccard_score_micro': 0.48853132488873674,\n",
              " 'macro-f1': 0.3781469942216553,\n",
              " 'micro-f1': 0.6563937442502299,\n",
              " 'Hamming Loss': 0.25236486486486487,\n",
              " 'Individual': {'0': {'accuracy': 0.706081081081081, 'f1': 0.6588235294117648},\n",
              "  '1': {'accuracy': 0.5878378378378378, 'f1': 0.17006802721088435},\n",
              "  '2': {'accuracy': 0.8817567567567568, 'f1': 0.0},\n",
              "  '3': {'accuracy': 0.7483108108108109, 'f1': 0.8396124865446718},\n",
              "  '4': {'accuracy': 0.8023648648648649, 'f1': 0.0},\n",
              "  '5': {'accuracy': 0.8733108108108109, 'f1': 0.0},\n",
              "  '6': {'accuracy': 0.5658783783783784, 'f1': 0.5834683954619125},\n",
              "  '7': {'accuracy': 0.6368243243243243, 'f1': 0.660347551342812},\n",
              "  '8': {'accuracy': 0.768581081081081, 'f1': 0.8691499522445081},\n",
              "  '9': {'accuracy': 0.9054054054054054, 'f1': 0.0}}}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    }
  ]
}