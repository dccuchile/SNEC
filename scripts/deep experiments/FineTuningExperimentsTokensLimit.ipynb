{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "FineTuningExperimentsServer.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZRcrOG1qTsz"
      },
      "source": [
        "!pip install --upgrade gensim\n",
        "!pip install transformers\n",
        "!pip install -U sentence-transformers\n",
        "!pip install pytorch-lightning==1.1.0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IhNbcBMqTs3"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score, f1_score, roc_auc_score, precision_score, classification_report, hamming_loss\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
        "\n",
        "from tabulate import tabulate\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from numba import cuda\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.metrics.functional.classification import auroc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from transformers import AutoModel, BertTokenizerFast, FeatureExtractionPipeline # BertTokenizer, BertModel, BertForSequenceClassification}\n",
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsNsPAkfqTs5"
      },
      "source": [
        "# cuda.select_device(0)\n",
        "# cuda.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMgf1hFH-wCr"
      },
      "source": [
        "## Cosas BETO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWUPVHlljf43"
      },
      "source": [
        "device = torch.device(\"cuda\") \n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
        "# def custom_tokenizer(text):\n",
        "#   return tokenizer(text, truncation=True, max_length=500)\n",
        "# model = AutoModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\", return_dict=True)\n",
        "\n",
        "# # freeze all the parameters\n",
        "# for param in model.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# model.eval()\n",
        "# BETO_features = FeatureExtractionPipeline(model, custom_tokenizer, device=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5OsuEBOglOA"
      },
      "source": [
        "text = [\"hola amiguitos\", \"bienvenidos a mi casa compañeros\"]\n",
        "sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)\n",
        "print(sent_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vtDtyoAK6Rn"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXRnCc5Pegy2"
      },
      "source": [
        "## Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2SZ11E0qTs-"
      },
      "source": [
        "categories_number_words = {\n",
        "        1: \"Apoyo Pedagógico en asignaturas\",\n",
        "        3: \"Apoyo pedagógico personal\",\n",
        "        4: \"Tutoría entre pares\",\n",
        "        7: \"Hacer a la familia partícipe del proceso\",\n",
        "        8: \"Apoyo psicóloga(o)\",\n",
        "        9: \"Apoyo fonoaudióloga(o)\",\n",
        "        10: \"Apoyo Educador(a) Diferencial\",\n",
        "        11: \"Apoyo Kinesióloga(o)\",\n",
        "        12: \"Apoyo Médico General\",\n",
        "        13: \"Apoyo Terapeuta Ocupacional\",\n",
        "        14: \"Control Neurólogo\",\n",
        "        15: \"Apoyo Interdisciplinario\",\n",
        "        16: \"Adecuación curricular de acceso\",\n",
        "        17: \"Adecuación curricular de objetivos\"\n",
        "    }\n",
        "categories_words_number = {v: k for k, v in categories_number_words.items()}\n",
        "\n",
        "diagnoses_codes = {\n",
        "    \"Trastorno específico del lenguaje\": 0,\n",
        "    \"Trastorno por déficit atencional\": 1,\n",
        "    \"Dificultad específica de aprendizaje\": 2,\n",
        "    \"Discapacidad intelectual\": 3,\n",
        "    \"Discapacidad visual\": 4,\n",
        "    \"Trastorno del espectro autista\": 5,\n",
        "    \"Discapacidad auditiva - Hipoacusia\": 6,\n",
        "    \"Funcionamiento intelectual limítrofe\": 7,\n",
        "    \"Síndrome de Down\": 8,\n",
        "    \"Trastorno motor\": 9,\n",
        "    \"Multidéficit\": 10,\n",
        "    \"Retraso global del desarrollo\": 11\n",
        "}\n",
        "\n",
        "diagnoses_keys = list(diagnoses_codes.keys())\n",
        "\n",
        "def transform_diag_to_array(code):\n",
        "    arr = np.zeros(len(diagnoses_keys), dtype=int)\n",
        "    for (index, label) in enumerate(diagnoses_keys):\n",
        "        if diagnoses_codes[label]==code:\n",
        "            arr[index] = 1\n",
        "    return arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3ch6J3ZqTs_"
      },
      "source": [
        "### Datos pre cargados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAWH2vr1Rj15"
      },
      "source": [
        "# students_strats = pd.read_csv('/home/jamunoz/datasets/anonimized_dataset.csv')\n",
        "# columns = students_strats.columns\n",
        "# for var in columns:\n",
        "#     if var != 'Diagnoses' and var != 'Index':\n",
        "#         students_strats[var] = students_strats[var].apply(ast.literal_eval)\n",
        "# students_strats.columns\n",
        "train_dataset = pd.read_csv('/home/jamunoz/datasets/train_ds.csv', keep_default_na=False)\n",
        "val_dataset = pd.read_csv('/home/jamunoz/datasets/val_ds.csv', keep_default_na=False)\n",
        "test_dataset = pd.read_csv('/home/jamunoz/datasets/test_ds.csv', keep_default_na=False)\n",
        "\n",
        "# Add OHE diagnosis\n",
        "train_OHE_diags = []\n",
        "for diag in train_dataset['Encoded Diagnosis']:\n",
        "    train_OHE_diags.append(transform_diag_to_array(diag))\n",
        "temp_train_diags_df = pd.DataFrame(train_OHE_diags, columns=diagnoses_keys)\n",
        "train_dataset = pd.concat([train_dataset, temp_train_diags_df], axis=1)\n",
        "\n",
        "val_OHE_diags = []\n",
        "for diag in val_dataset['Encoded Diagnosis']:\n",
        "    val_OHE_diags.append(transform_diag_to_array(diag))\n",
        "temp_val_diags_df = pd.DataFrame(val_OHE_diags, columns=diagnoses_keys)\n",
        "val_dataset = pd.concat([val_dataset, temp_val_diags_df], axis=1)\n",
        "\n",
        "test_OHE_diags = []\n",
        "for diag in test_dataset['Encoded Diagnosis']:\n",
        "    test_OHE_diags.append(transform_diag_to_array(diag))\n",
        "temp_test_diags_df = pd.DataFrame(test_OHE_diags, columns=diagnoses_keys)\n",
        "test_dataset = pd.concat([test_dataset, temp_test_diags_df], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOjWWHCOqTtA"
      },
      "source": [
        "train_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMER3ENIqTtB"
      },
      "source": [
        "## Experimentos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKl49oD67ApW"
      },
      "source": [
        "# y_keys = list(strat_present.keys())\n",
        "Y_KEYS = list(categories_words_number.keys())\n",
        "\n",
        "# df = pd.DataFrame(data=new_dataset_to_export)\n",
        "# X = df\n",
        "# Y = df[y_keys]\n",
        "X_train = train_dataset.drop(Y_KEYS, axis=1)\n",
        "Y_train = train_dataset[Y_KEYS]\n",
        "X_val = val_dataset.drop(Y_KEYS, axis=1)\n",
        "Y_val = val_dataset[Y_KEYS]\n",
        "\n",
        "strats_amounts = {\n",
        "              'Adecuación curricular de acceso': 2264,\n",
        "              'Hacer a la familia partícipe del proceso': 2048,\n",
        "              'Apoyo Interdisciplinario': 1441, \n",
        "              'Apoyo Educador(a) Diferencial': 1311,\n",
        "              'Apoyo pedagógico personal': 1240,\n",
        "              'Apoyo fonoaudióloga(o)': 378,\n",
        "              'Apoyo psicóloga(o)': 588,\n",
        "              'Apoyo Terapeuta Ocupacional': 153,\n",
        "              'Tutoría entre pares': 350,\n",
        "              'Control Neurólogo': 63,\n",
        "              'Apoyo Médico General': 64,\n",
        "              'Apoyo Kinesióloga(o)': 32,\n",
        "              'Adecuación curricular de objetivos': 281,\n",
        "              'Apoyo Pedagógico en asignaturas': 1314\n",
        "}\n",
        "most_unbalanced_strategies = [strategy for strategy in Y_KEYS if (\n",
        "    strats_amounts[strategy] < (len(X_train) + len(X_val))*0.15 or strats_amounts[strategy] > (len(X_train) + len(X_val))*0.85)]\n",
        "less_unbalanced_strategies = [strategy for strategy in Y_KEYS if strategy not in most_unbalanced_strategies]\n",
        "only_one_strat = [Y_KEYS[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJDstZIbqTtC"
      },
      "source": [
        "X_train.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwDZPJ2F8c9O"
      },
      "source": [
        "percs = X_train['All perceptions']\n",
        "seq_len = [len(i.split()) for i in percs]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbTReFZtX5Uj"
      },
      "source": [
        "print(0 in seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AjleXRkYRtT"
      },
      "source": [
        "percs = X_train['Special Education Teacher Perceptions']\n",
        "seq_len = [len(i.split()) for i in percs]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-riyhou8X-A3"
      },
      "source": [
        "print(0 in seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a9_ig9iZx4I"
      },
      "source": [
        "percs = X_train['Speech Therapist Perceptions']\n",
        "seq_len = [len(i.split()) for i in percs]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrtYSTezZyLL"
      },
      "source": [
        "percs = X_train['Psychologist Perceptions']\n",
        "seq_len = [len(i.split()) for i in percs]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVTFgirwZyoP"
      },
      "source": [
        "percs = X_train['Medical Perceptions']\n",
        "seq_len = [len(i.split()) for i in percs]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG9FEovFbobu"
      },
      "source": [
        "# sentence_model = SentenceTransformer('xlm-r-bert-base-nli-stsb-mean-tokens', device='cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWJNtAmJsb8y"
      },
      "source": [
        "# sentence_model.max_seq_length = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRUFdCNAgR-Q"
      },
      "source": [
        "# sentence_model.encode(X['All perceptions'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Cx6j3O8LNXV"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPmcH6vNilj5"
      },
      "source": [
        "## Transformación de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4SyhPU4qTtH"
      },
      "source": [
        "# class JoinedProfessionalPerceptionsDataset(Dataset):\n",
        "    \n",
        "#     def __init__(self, data: pd.DataFrame, tokenizer: BertTokenizerFast, max_token_len=200):\n",
        "#         self.data = data\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.max_token_len = max_token_len\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "    \n",
        "#     def __getitem__(self, index: int):\n",
        "        \n",
        "#         data_row = self.data.iloc[index]\n",
        "        \n",
        "#         text = data_row['All perceptions']    \n",
        "#         labels =  data_row[Y_KEYS]\n",
        "        \n",
        "#         encoding = self.tokenizer.encode_plus(\n",
        "#             text,\n",
        "#             # add_special_tokens=True,\n",
        "#             max_length=self.max_token_len,\n",
        "#             padding='max_length',\n",
        "#             truncation=True,\n",
        "#             return_token_type_ids=False,\n",
        "#             return_attention_mask=True,\n",
        "#             return_tensors='pt'\n",
        "#         )\n",
        "        \n",
        "#         return dict(\n",
        "#             text=text,\n",
        "#             labels=torch.FloatTensor(labels),\n",
        "#             # posiblemente cambiar porque el modelo recibe el tensor agrupado\n",
        "#             input_ids=encoding[\"input_ids\"].flatten(),\n",
        "#             attention_mask=encoding[\"attention_mask\"].flatten()\n",
        "#         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVzU0bxkqTtH"
      },
      "source": [
        "class DiagnosisAndDividedPerceptionsDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, data: pd.DataFrame, tokenizer: BertTokenizerFast, max_token_len=200):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_token_len = max_token_len\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index: int):\n",
        "        \n",
        "        data_row = self.data.iloc[index]\n",
        "        \n",
        "        sne_teacher_obs = data_row['Special Education Teacher Perceptions']\n",
        "        speech_therapist_obs = data_row['Speech Therapist Perceptions']\n",
        "        psychologist_obs = data_row['Psychologist Perceptions']\n",
        "        medical_obs = data_row['Medical Perceptions']\n",
        "        \n",
        "        labels =  data_row[Y_KEYS]\n",
        "        \n",
        "        encoding_sne_obs = self.tokenizer.encode_plus(\n",
        "            sne_teacher_obs,\n",
        "            # add_special_tokens=True,\n",
        "            max_length=self.max_token_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_token_type_ids=False,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        encoding_speech_obs = self.tokenizer.encode_plus(\n",
        "            speech_therapist_obs,\n",
        "            # add_special_tokens=True,\n",
        "            max_length=self.max_token_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_token_type_ids=False,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        encoding_psychologist_obs = self.tokenizer.encode_plus(\n",
        "            psychologist_obs,\n",
        "            # add_special_tokens=True,\n",
        "            max_length=self.max_token_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_token_type_ids=False,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        encoding_medical_obs = self.tokenizer.encode_plus(\n",
        "            medical_obs,\n",
        "            # add_special_tokens=True,\n",
        "            max_length=self.max_token_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_token_type_ids=False,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        encoding_diagnosis = self.tokenizer.encode_plus(\n",
        "            medical_obs,\n",
        "            # add_special_tokens=True,\n",
        "            max_length=30,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_token_type_ids=False,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        dict_to_return = dict(\n",
        "            sne_text=sne_teacher_obs,\n",
        "            st_text=speech_therapist_obs,\n",
        "            p_text=psychologist_obs,\n",
        "            m_text=medical_obs,\n",
        "            labels=torch.FloatTensor(labels),\n",
        "            # posiblemente cambiar porque el modelo recibe el tensor agrupado\n",
        "            sne_input_ids=encoding_sne_obs[\"input_ids\"].flatten(),\n",
        "            sne_attention_mask=encoding_sne_obs[\"attention_mask\"].flatten(),\n",
        "            st_input_ids=encoding_speech_obs[\"input_ids\"].flatten(),\n",
        "            st_attention_mask=encoding_speech_obs[\"attention_mask\"].flatten(),\n",
        "            p_input_ids=encoding_psychologist_obs[\"input_ids\"].flatten(),\n",
        "            p_attention_mask=encoding_psychologist_obs[\"attention_mask\"].flatten(),\n",
        "            m_input_ids=encoding_medical_obs[\"input_ids\"].flatten(),\n",
        "            m_attention_mask=encoding_medical_obs[\"attention_mask\"].flatten(),\n",
        "            diagnosis=data_row[diagnoses_keys].to_numpy(dtype=\"int\"),\n",
        "            diagnosis_text=data_row['Diagnosis'],\n",
        "            diagnosis_ids=encoding_diagnosis[\"input_ids\"].flatten(),\n",
        "            diagnosis_attention_mask=encoding_diagnosis[\"attention_mask\"].flatten(),\n",
        "        )\n",
        "        \n",
        "        return dict_to_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5q2E1sdqTtJ"
      },
      "source": [
        "trans_sample = DiagnosisAndDividedPerceptionsDataset(train_dataset, tokenizer)\n",
        "sample_item = trans_sample[0]\n",
        "sample_item['diagnosis']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDd2O4ZMqTtL"
      },
      "source": [
        "# transformed_train_ds = JoinedProfessionalPerceptionsDataset(train_dataset, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFN6kMr2qTtM"
      },
      "source": [
        "# sample_item = transformed_train_ds[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQVC-tCrqTtN"
      },
      "source": [
        "# sample_item[\"text\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-PCn6z8qTtN"
      },
      "source": [
        "# sample_item[\"labels\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kGXAOSWqTtN"
      },
      "source": [
        "# sample_item[\"input_ids\"].size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT15X13FqTtO"
      },
      "source": [
        "# class JoinedProfessionalPerceptionsDataModule(pl.LightningDataModule):\n",
        "    \n",
        "#     def __init__(self, train_df, val_df, test_df, tokenizer, batch_size=8, max_token_len=200):\n",
        "#         super().__init__()\n",
        "#         self.train_df = train_df\n",
        "#         self.val_df = val_df\n",
        "#         self.test_df = test_df\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.batch_size = batch_size\n",
        "#         self.max_token_len = max_token_len\n",
        "        \n",
        "#     def setup(self):\n",
        "#         self.train_dataset = JoinedProfessionalPerceptionsDataset(\n",
        "#             self.train_df,\n",
        "#             self.tokenizer,\n",
        "#             self.max_token_len\n",
        "#         )\n",
        "        \n",
        "#         self.val_dataset = JoinedProfessionalPerceptionsDataset(\n",
        "#             self.val_df,\n",
        "#             self.tokenizer,\n",
        "#             self.max_token_len\n",
        "#         )\n",
        "        \n",
        "#         self.test_dataset = JoinedProfessionalPerceptionsDataset(\n",
        "#             self.test_df,\n",
        "#             self.tokenizer,\n",
        "#             self.max_token_len\n",
        "#         )\n",
        "        \n",
        "#     def train_dataloader(self):\n",
        "#         return DataLoader(\n",
        "#             self.train_dataset,\n",
        "#             batch_size=self.batch_size,\n",
        "#             shuffle=True,\n",
        "#             num_workers=4\n",
        "#         )\n",
        "    \n",
        "#     def val_dataloader(self):\n",
        "#         return DataLoader(\n",
        "#             self.val_dataset,\n",
        "#             batch_size=1,\n",
        "#             num_workers=4\n",
        "#         )\n",
        "    \n",
        "#     def test_dataloader(self):\n",
        "#         return DataLoader(\n",
        "#             self.test_dataset,\n",
        "#             batch_size=1,\n",
        "#             num_workers=4\n",
        "#         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z852WKIpqTtP"
      },
      "source": [
        "class DiagnosisAndDividedPerceptionsDataModule(pl.LightningDataModule):\n",
        "    \n",
        "    def __init__(self, train_df, val_df, test_df, tokenizer, batch_size=8, max_token_len=200):\n",
        "        super().__init__()\n",
        "        self.train_df = train_df\n",
        "        self.val_df = val_df\n",
        "        self.test_df = test_df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.batch_size = batch_size\n",
        "        self.max_token_len = max_token_len\n",
        "        \n",
        "    def setup(self):\n",
        "        self.train_dataset = DiagnosisAndDividedPerceptionsDataset(\n",
        "            self.train_df,\n",
        "            self.tokenizer,\n",
        "            self.max_token_len\n",
        "        )\n",
        "        \n",
        "        self.val_dataset = DiagnosisAndDividedPerceptionsDataset(\n",
        "            self.val_df,\n",
        "            self.tokenizer,\n",
        "            self.max_token_len\n",
        "        )\n",
        "        \n",
        "        self.test_dataset = DiagnosisAndDividedPerceptionsDataset(\n",
        "            self.test_df,\n",
        "            self.tokenizer,\n",
        "            self.max_token_len\n",
        "        )\n",
        "        \n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=4\n",
        "        )\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=1,\n",
        "            num_workers=4\n",
        "        )\n",
        "    \n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=1,\n",
        "            num_workers=4\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN5AwQrZqTtQ"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "BATCH_SIZE = 2\n",
        "# special batch size when using 500 tokens limit\n",
        "SPECIAL_BATCH_SIZE = 1\n",
        "\n",
        "# data_module = JoinedProfessionalPerceptionsDataModule(train_dataset, val_dataset, test_dataset, tokenizer, batch_size=BATCH_SIZE)\n",
        "# data_module.setup()\n",
        "# Tried to use different BERT for each perception type but was unable due to run out of memory\n",
        "\n",
        "# data_module_100 = DiagnosisAndDividedPerceptionsDataModule(\n",
        "#     train_dataset, \n",
        "#     val_dataset, \n",
        "#     test_dataset, \n",
        "#     tokenizer, \n",
        "#     batch_size=BATCH_SIZE,\n",
        "#     max_token_len=100\n",
        "# )\n",
        "# data_module_100.setup()\n",
        "\n",
        "\n",
        "data_module_200 = DiagnosisAndDividedPerceptionsDataModule(\n",
        "    train_dataset, \n",
        "    val_dataset, \n",
        "    test_dataset, \n",
        "    tokenizer, \n",
        "    batch_size=BATCH_SIZE,\n",
        "    max_token_len=200\n",
        ")\n",
        "data_module_200.setup()\n",
        "\n",
        "data_module_300 = DiagnosisAndDividedPerceptionsDataModule(\n",
        "    train_dataset, \n",
        "    val_dataset, \n",
        "    test_dataset, \n",
        "    tokenizer, \n",
        "    batch_size=BATCH_SIZE,\n",
        "    max_token_len=300\n",
        ")\n",
        "data_module_300.setup()\n",
        "\n",
        "data_module_400 = DiagnosisAndDividedPerceptionsDataModule(\n",
        "    train_dataset, \n",
        "    val_dataset, \n",
        "    test_dataset, \n",
        "    tokenizer, \n",
        "    batch_size=SPECIAL_BATCH_SIZE,\n",
        "    max_token_len=400\n",
        ")\n",
        "data_module_400.setup()\n",
        "\n",
        "data_module_500 = DiagnosisAndDividedPerceptionsDataModule(\n",
        "    train_dataset, \n",
        "    val_dataset, \n",
        "    test_dataset, \n",
        "    tokenizer, \n",
        "    batch_size=SPECIAL_BATCH_SIZE,\n",
        "    max_token_len=500\n",
        ")\n",
        "data_module_500.setup()\n",
        "\n",
        "mini_200_module = DiagnosisAndDividedPerceptionsDataModule(\n",
        "    train_dataset, \n",
        "    val_dataset, \n",
        "    test_dataset, \n",
        "    tokenizer, \n",
        "    batch_size=SPECIAL_BATCH_SIZE,\n",
        "    max_token_len=200\n",
        ")\n",
        "mini_200_module.setup()\n",
        "\n",
        "mini_300_module = DiagnosisAndDividedPerceptionsDataModule(\n",
        "    train_dataset, \n",
        "    val_dataset, \n",
        "    test_dataset, \n",
        "    tokenizer, \n",
        "    batch_size=SPECIAL_BATCH_SIZE,\n",
        "    max_token_len=300\n",
        ")\n",
        "mini_300_module.setup()\n",
        "\n",
        "mini_400_module = DiagnosisAndDividedPerceptionsDataModule(\n",
        "    train_dataset, \n",
        "    val_dataset, \n",
        "    test_dataset, \n",
        "    tokenizer, \n",
        "    batch_size=SPECIAL_BATCH_SIZE,\n",
        "    max_token_len=400\n",
        ")\n",
        "mini_400_module.setup()\n",
        "\n",
        "mini_500_module = DiagnosisAndDividedPerceptionsDataModule(\n",
        "    train_dataset, \n",
        "    val_dataset, \n",
        "    test_dataset, \n",
        "    tokenizer, \n",
        "    batch_size=SPECIAL_BATCH_SIZE,\n",
        "    max_token_len=500\n",
        ")\n",
        "mini_500_module.setup()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xHieCfYqTtX"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMjnWqJDqTtX"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9LGYoVpqTtX"
      },
      "source": [
        "# class JoinedObservationsClassifier(pl.LightningModule):\n",
        "    \n",
        "#     def __init__(self, n_classes: int, steps_per_epoch=None, n_epochs=None):\n",
        "#         super().__init__()\n",
        "#         self.bert = AutoModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\", return_dict=True)\n",
        "#         self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "#         self.steps_per_epoch = steps_per_epoch\n",
        "#         self.n_epochs = n_epochs\n",
        "#         self.criterion = nn.BCELoss()\n",
        "        \n",
        "#     def forward(self, inputs_ids, attention_mask, labels=None):\n",
        "#         output = self.bert(inputs_ids, attention_mask=attention_mask)\n",
        "#         output = self.classifier(output.pooler_output)\n",
        "#         output = torch.sigmoid(output)\n",
        "#         loss = 0\n",
        "#         if labels is not None:\n",
        "#             loss = self.criterion(output, labels)\n",
        "#         return loss, output\n",
        "    \n",
        "#     def training_step(self, batch, batch_idx):\n",
        "#         input_ids = batch['input_ids']\n",
        "#         attention_mask = batch['attention_mask']\n",
        "#         labels = batch['labels']\n",
        "#         loss, outputs = self(input_ids, attention_mask, labels)\n",
        "#         self.log(\"train loss\", loss, prog_bar=True, logger=True)\n",
        "#         return {'loss': loss, 'predictions': outputs, 'labels': labels}\n",
        "    \n",
        "#     def validation_step(self, batch, batch_idx):\n",
        "#         input_ids = batch['input_ids']\n",
        "#         attention_mask = batch['attention_mask']\n",
        "#         labels = batch['labels']\n",
        "#         loss, outputs = self(input_ids, attention_mask, labels)\n",
        "#         self.log(\"val loss\", loss, prog_bar=True, logger=True)\n",
        "#         return loss\n",
        "    \n",
        "#     def test_step(self, batch, batch_idx):\n",
        "#         input_ids = batch['input_ids']\n",
        "#         attention_mask = batch['attention_mask']\n",
        "#         labels = batch['labels']\n",
        "#         loss, outputs = self(input_ids, attention_mask, labels)\n",
        "#         self.log(\"test loss\", loss, prog_bar=True, logger=True)\n",
        "#         return loss\n",
        "    \n",
        "#     def training_epoch_end(self, outputs):\n",
        "#         labels = []\n",
        "#         predictions = []\n",
        "        \n",
        "#         for output in outputs:\n",
        "#             for out_labels in output['labels'].detach().cpu():\n",
        "#                 labels.append(out_labels)\n",
        "                \n",
        "#             for out_predictions in output['predictions'].detach().cpu():\n",
        "#                 predictions.append(out_predictions)\n",
        "                \n",
        "#         labels = torch.stack(labels)\n",
        "#         predictions = torch.stack(predictions)\n",
        "        \n",
        "#         for i, name in enumerate(Y_KEYS):\n",
        "#             roc_score = auroc(predictions[:, i], labels[:, i])\n",
        "#             self.logger.experiment.add_scalar(f\"{name}__roc_auc/Train\", roc_score, self.current_epoch)\n",
        "            \n",
        "#     def configure_optimizers(self): \n",
        "#         optimizer = AdamW(self.parameters(), lr=2e-5)\n",
        "        \n",
        "#         warmup_steps = self.steps_per_epoch // 3\n",
        "#         total_steps = self.steps_per_epoch * self.n_epochs - warmup_steps\n",
        "        \n",
        "#         scheduler = get_linear_schedule_with_warmup(\n",
        "#             optimizer,\n",
        "#             warmup_steps,\n",
        "#             total_steps\n",
        "#         )\n",
        "        \n",
        "#         return [optimizer], [scheduler] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "363xyuJhqTtY"
      },
      "source": [
        "class DiagnosisAndDividedObservationsClassifier(pl.LightningModule):\n",
        "    \n",
        "    def __init__(self, n_classes: int, n_diags: int, steps_per_epoch=None, n_epochs=None):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\", return_dict=True)\n",
        "        # self.diagnosis_layer = nn.Linear(1, 10)\n",
        "        self.classifier = nn.Linear((self.bert.config.hidden_size*4)+n_diags, n_classes)\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "        self.n_epochs = n_epochs\n",
        "        self.criterion = nn.BCELoss()\n",
        "        \n",
        "    def forward(self, \n",
        "                sne_inputs_ids, sne_attention_mask,\n",
        "                st_inputs_ids, st_attention_mask,\n",
        "                p_inputs_ids, p_attention_mask,\n",
        "                m_inputs_ids, m_attention_mask,\n",
        "                diagnosis, labels=None):\n",
        "        # print(diagnosis)\n",
        "        output_sne = self.bert(sne_inputs_ids, attention_mask=sne_attention_mask)\n",
        "        output_st = self.bert(st_inputs_ids, attention_mask=st_attention_mask)\n",
        "        output_p = self.bert(p_inputs_ids, attention_mask=p_attention_mask)\n",
        "        output_m = self.bert(m_inputs_ids, attention_mask=m_attention_mask)\n",
        "        # output_diagnosis = self.diagnosis_layer(torch.tensor([int(diagnosis)]))\n",
        "        # diags_tensor = torch.from_numpy(diagnosis)\n",
        "        output = torch.cat(\n",
        "            (output_sne.pooler_output,\n",
        "             output_st.pooler_output,\n",
        "             output_p.pooler_output,\n",
        "             output_m.pooler_output,\n",
        "             diagnosis #_tensor,\n",
        "            ), dim=1)\n",
        "        output = self.classifier(output)\n",
        "        output = torch.sigmoid(output)\n",
        "        loss = 0\n",
        "        if labels is not None:\n",
        "            loss = self.criterion(output, labels)\n",
        "        return loss, output\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        sne_input_ids = batch['sne_input_ids']\n",
        "        sne_attention_mask = batch['sne_attention_mask']\n",
        "        st_input_ids = batch['st_input_ids']\n",
        "        st_attention_mask = batch['st_attention_mask']\n",
        "        p_input_ids = batch['p_input_ids']\n",
        "        p_attention_mask = batch['p_attention_mask']\n",
        "        m_input_ids = batch['m_input_ids']\n",
        "        m_attention_mask = batch['m_attention_mask']\n",
        "        diagnosis = batch['diagnosis']\n",
        "        labels = batch['labels']\n",
        "        loss, outputs = self(\n",
        "            sne_input_ids, sne_attention_mask,\n",
        "            st_input_ids, st_attention_mask,\n",
        "            p_input_ids, p_attention_mask,\n",
        "            m_input_ids, m_attention_mask,\n",
        "            diagnosis, labels)\n",
        "        self.log(\"train loss\", loss, prog_bar=True, logger=True)\n",
        "        return {'loss': loss, 'predictions': outputs, 'labels': labels}\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        sne_input_ids = batch['sne_input_ids']\n",
        "        sne_attention_mask = batch['sne_attention_mask']\n",
        "        st_input_ids = batch['st_input_ids']\n",
        "        st_attention_mask = batch['st_attention_mask']\n",
        "        p_input_ids = batch['p_input_ids']\n",
        "        p_attention_mask = batch['p_attention_mask']\n",
        "        m_input_ids = batch['m_input_ids']\n",
        "        m_attention_mask = batch['m_attention_mask']\n",
        "        diagnosis = batch['diagnosis']\n",
        "        labels = batch['labels']\n",
        "        loss, outputs = self(\n",
        "            sne_input_ids, sne_attention_mask,\n",
        "            st_input_ids, st_attention_mask,\n",
        "            p_input_ids, p_attention_mask,\n",
        "            m_input_ids, m_attention_mask,\n",
        "            diagnosis, labels)\n",
        "        self.log(\"val loss\", loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        sne_input_ids = batch['sne_input_ids']\n",
        "        sne_attention_mask = batch['sne_attention_mask']\n",
        "        st_input_ids = batch['st_input_ids']\n",
        "        st_attention_mask = batch['st_attention_mask']\n",
        "        p_attention_mask = batch['p_attention_mask']\n",
        "        m_input_ids = batch['m_input_ids']\n",
        "        m_attention_mask = batch['m_attention_mask']\n",
        "        diagnosis = batch['diagnosis']\n",
        "        labels = batch['labels']\n",
        "        loss, outputs = self(\n",
        "            sne_input_ids, sne_attention_mask,\n",
        "            st_input_ids, st_attention_mask,\n",
        "            p_input_ids, p_attention_mask,\n",
        "            m_input_ids, m_attention_mask,\n",
        "            diagnosis, labels)\n",
        "        self.log(\"test loss\", loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "    \n",
        "    def training_epoch_end(self, outputs):\n",
        "        labels = []\n",
        "        predictions = []\n",
        "        \n",
        "        for output in outputs:\n",
        "            for out_labels in output['labels'].detach().cpu():\n",
        "                labels.append(out_labels)\n",
        "                \n",
        "            for out_predictions in output['predictions'].detach().cpu():\n",
        "                predictions.append(out_predictions)\n",
        "                \n",
        "        labels = torch.stack(labels)\n",
        "        predictions = torch.stack(predictions)\n",
        "        \n",
        "        for i, name in enumerate(Y_KEYS):\n",
        "            roc_score = auroc(predictions[:, i], labels[:, i])\n",
        "            self.logger.experiment.add_scalar(f\"{name}__roc_auc/Train\", roc_score, self.current_epoch)\n",
        "            \n",
        "    def configure_optimizers(self): \n",
        "        optimizer = AdamW(self.parameters(), lr=2e-5)\n",
        "        \n",
        "        warmup_steps = self.steps_per_epoch // 3\n",
        "        total_steps = self.steps_per_epoch * self.n_epochs - warmup_steps\n",
        "        \n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            warmup_steps,\n",
        "            total_steps\n",
        "        )\n",
        "        \n",
        "        return [optimizer], [scheduler] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgeN9M-gqTtY"
      },
      "source": [
        "class DiagnosisAndDividedObservationsClassifierExtraLayer(pl.LightningModule):\n",
        "    \n",
        "    def __init__(self, n_classes: int, n_diags: int, steps_per_epoch=None, n_epochs=None):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\", return_dict=True)\n",
        "        self.new_layer = nn.Linear((self.bert.config.hidden_size*4), 512)\n",
        "        self.classifier = nn.Linear(512+n_diags, n_classes)\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "        self.n_epochs = n_epochs\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, \n",
        "                sne_inputs_ids, sne_attention_mask,\n",
        "                st_inputs_ids, st_attention_mask,\n",
        "                p_inputs_ids, p_attention_mask,\n",
        "                m_inputs_ids, m_attention_mask,\n",
        "                diagnosis, labels=None):\n",
        "        # print(diagnosis)\n",
        "        output_sne = self.bert(sne_inputs_ids, attention_mask=sne_attention_mask)\n",
        "        output_st = self.bert(st_inputs_ids, attention_mask=st_attention_mask)\n",
        "        output_p = self.bert(p_inputs_ids, attention_mask=p_attention_mask)\n",
        "        output_m = self.bert(m_inputs_ids, attention_mask=m_attention_mask)\n",
        "        # output_diagnosis = self.diagnosis_layer(torch.tensor([int(diagnosis)]))\n",
        "        # diags_tensor = torch.from_numpy(diagnosis)\n",
        "        output = torch.cat(\n",
        "            (output_sne.pooler_output,\n",
        "             output_st.pooler_output,\n",
        "             output_p.pooler_output,\n",
        "             output_m.pooler_output,\n",
        "            ), dim=1)\n",
        "        output = self.new_layer(output)\n",
        "        output = self.relu(output)\n",
        "        output = self.dropout(output)\n",
        "        output = torch.cat(\n",
        "            (\n",
        "                output,\n",
        "                diagnosis\n",
        "            ), dim=1\n",
        "        )\n",
        "        output = self.classifier(output)\n",
        "        output = torch.sigmoid(output)\n",
        "        loss = 0\n",
        "        if labels is not None:\n",
        "            loss = self.criterion(output, labels)\n",
        "        return loss, output\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        sne_input_ids = batch['sne_input_ids']\n",
        "        sne_attention_mask = batch['sne_attention_mask']\n",
        "        st_input_ids = batch['st_input_ids']\n",
        "        st_attention_mask = batch['st_attention_mask']\n",
        "        p_input_ids = batch['p_input_ids']\n",
        "        p_attention_mask = batch['p_attention_mask']\n",
        "        m_input_ids = batch['m_input_ids']\n",
        "        m_attention_mask = batch['m_attention_mask']\n",
        "        diagnosis = batch['diagnosis']\n",
        "        labels = batch['labels']\n",
        "        loss, outputs = self(\n",
        "            sne_input_ids, sne_attention_mask,\n",
        "            st_input_ids, st_attention_mask,\n",
        "            p_input_ids, p_attention_mask,\n",
        "            m_input_ids, m_attention_mask,\n",
        "            diagnosis, labels)\n",
        "        self.log(\"train loss\", loss, prog_bar=True, logger=True)\n",
        "        return {'loss': loss, 'predictions': outputs, 'labels': labels}\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        sne_input_ids = batch['sne_input_ids']\n",
        "        sne_attention_mask = batch['sne_attention_mask']\n",
        "        st_input_ids = batch['st_input_ids']\n",
        "        st_attention_mask = batch['st_attention_mask']\n",
        "        p_input_ids = batch['p_input_ids']\n",
        "        p_attention_mask = batch['p_attention_mask']\n",
        "        m_input_ids = batch['m_input_ids']\n",
        "        m_attention_mask = batch['m_attention_mask']\n",
        "        diagnosis = batch['diagnosis']\n",
        "        labels = batch['labels']\n",
        "        loss, outputs = self(\n",
        "            sne_input_ids, sne_attention_mask,\n",
        "            st_input_ids, st_attention_mask,\n",
        "            p_input_ids, p_attention_mask,\n",
        "            m_input_ids, m_attention_mask,\n",
        "            diagnosis, labels)\n",
        "        self.log(\"val loss\", loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        sne_input_ids = batch['sne_input_ids']\n",
        "        sne_attention_mask = batch['sne_attention_mask']\n",
        "        st_input_ids = batch['st_input_ids']\n",
        "        st_attention_mask = batch['st_attention_mask']\n",
        "        p_attention_mask = batch['p_attention_mask']\n",
        "        m_input_ids = batch['m_input_ids']\n",
        "        m_attention_mask = batch['m_attention_mask']\n",
        "        diagnosis = batch['diagnosis']\n",
        "        labels = batch['labels']\n",
        "        loss, outputs = self(\n",
        "            sne_input_ids, sne_attention_mask,\n",
        "            st_input_ids, st_attention_mask,\n",
        "            p_input_ids, p_attention_mask,\n",
        "            m_input_ids, m_attention_mask,\n",
        "            diagnosis, labels)\n",
        "        self.log(\"test loss\", loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "    \n",
        "    def training_epoch_end(self, outputs):\n",
        "        labels = []\n",
        "        predictions = []\n",
        "        \n",
        "        for output in outputs:\n",
        "            for out_labels in output['labels'].detach().cpu():\n",
        "                labels.append(out_labels)\n",
        "                \n",
        "            for out_predictions in output['predictions'].detach().cpu():\n",
        "                predictions.append(out_predictions)\n",
        "                \n",
        "        labels = torch.stack(labels)\n",
        "        predictions = torch.stack(predictions)\n",
        "        \n",
        "        for i, name in enumerate(Y_KEYS):\n",
        "            roc_score = auroc(predictions[:, i], labels[:, i])\n",
        "            self.logger.experiment.add_scalar(f\"{name}__roc_auc/Train\", roc_score, self.current_epoch)\n",
        "            \n",
        "    def configure_optimizers(self): \n",
        "        optimizer = AdamW(self.parameters(), lr=2e-5)\n",
        "        \n",
        "        warmup_steps = self.steps_per_epoch // 3\n",
        "        total_steps = self.steps_per_epoch * self.n_epochs - warmup_steps\n",
        "        \n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            warmup_steps,\n",
        "            total_steps\n",
        "        )\n",
        "        \n",
        "        return [optimizer], [scheduler] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3zJDuU5qTtZ"
      },
      "source": [
        "class DiagnosisAndDividedObservationsClassifierBertForDiag(pl.LightningModule):\n",
        "    \n",
        "    def __init__(self, n_classes: int, n_diags: int, steps_per_epoch=None, n_epochs=None):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\", return_dict=True)\n",
        "        self.bert_diag = AutoModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\", return_dict=True)\n",
        "        self.classifier = nn.Linear(\n",
        "            self.bert.config.hidden_size*4+self.bert_diag.config.hidden_size, n_classes)\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "        self.n_epochs = n_epochs\n",
        "        self.criterion = nn.BCELoss()\n",
        "        \n",
        "    def forward(self, \n",
        "                sne_inputs_ids, sne_attention_mask,\n",
        "                st_inputs_ids, st_attention_mask,\n",
        "                p_inputs_ids, p_attention_mask,\n",
        "                m_inputs_ids, m_attention_mask,\n",
        "                diagnosis_inputs_ids, diagnosis_attention_mask,\n",
        "                labels=None):\n",
        "        # print(diagnosis)\n",
        "        output_sne = self.bert(sne_inputs_ids, attention_mask=sne_attention_mask)\n",
        "        output_st = self.bert(st_inputs_ids, attention_mask=st_attention_mask)\n",
        "        output_p = self.bert(p_inputs_ids, attention_mask=p_attention_mask)\n",
        "        output_m = self.bert(m_inputs_ids, attention_mask=m_attention_mask)\n",
        "        output_diag = self.bert_diag(diagnosis_inputs_ids, attention_mask=diagnosis_attention_mask)\n",
        "        # output_diagnosis = self.diagnosis_layer(torch.tensor([int(diagnosis)]))\n",
        "        # diags_tensor = torch.from_numpy(diagnosis)\n",
        "        output = torch.cat(\n",
        "            (output_sne.pooler_output,\n",
        "             output_st.pooler_output,\n",
        "             output_p.pooler_output,\n",
        "             output_m.pooler_output,\n",
        "             output_diag.pooler_output,\n",
        "            ), dim=1)\n",
        "        output = self.classifier(output)\n",
        "        output = torch.sigmoid(output)\n",
        "        loss = 0\n",
        "        if labels is not None:\n",
        "            loss = self.criterion(output, labels)\n",
        "        return loss, output\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        sne_input_ids = batch['sne_input_ids']\n",
        "        sne_attention_mask = batch['sne_attention_mask']\n",
        "        st_input_ids = batch['st_input_ids']\n",
        "        st_attention_mask = batch['st_attention_mask']\n",
        "        p_input_ids = batch['p_input_ids']\n",
        "        p_attention_mask = batch['p_attention_mask']\n",
        "        m_input_ids = batch['m_input_ids']\n",
        "        m_attention_mask = batch['m_attention_mask']\n",
        "        diagnosis_ids = batch['diagnosis_ids']\n",
        "        diagnosis_attention_mask = batch['diagnosis_attention_mask']\n",
        "        labels = batch['labels']\n",
        "        loss, outputs = self(\n",
        "            sne_input_ids, sne_attention_mask,\n",
        "            st_input_ids, st_attention_mask,\n",
        "            p_input_ids, p_attention_mask,\n",
        "            m_input_ids, m_attention_mask,\n",
        "            diagnosis_ids, diagnosis_attention_mask, labels)\n",
        "        self.log(\"train loss\", loss, prog_bar=True, logger=True)\n",
        "        return {'loss': loss, 'predictions': outputs, 'labels': labels}\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        sne_input_ids = batch['sne_input_ids']\n",
        "        sne_attention_mask = batch['sne_attention_mask']\n",
        "        st_input_ids = batch['st_input_ids']\n",
        "        st_attention_mask = batch['st_attention_mask']\n",
        "        p_input_ids = batch['p_input_ids']\n",
        "        p_attention_mask = batch['p_attention_mask']\n",
        "        m_input_ids = batch['m_input_ids']\n",
        "        m_attention_mask = batch['m_attention_mask']\n",
        "        diagnosis_ids = batch['diagnosis_ids']\n",
        "        diagnosis_attention_mask = batch['diagnosis_attention_mask']\n",
        "        labels = batch['labels']\n",
        "        loss, outputs = self(\n",
        "            sne_input_ids, sne_attention_mask,\n",
        "            st_input_ids, st_attention_mask,\n",
        "            p_input_ids, p_attention_mask,\n",
        "            m_input_ids, m_attention_mask,\n",
        "            diagnosis_ids, diagnosis_attention_mask, labels)\n",
        "        self.log(\"val loss\", loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        sne_input_ids = batch['sne_input_ids']\n",
        "        sne_attention_mask = batch['sne_attention_mask']\n",
        "        st_input_ids = batch['st_input_ids']\n",
        "        st_attention_mask = batch['st_attention_mask']\n",
        "        p_input_ids = batch['p_input_ids']\n",
        "        p_attention_mask = batch['p_attention_mask']\n",
        "        m_input_ids = batch['m_input_ids']\n",
        "        m_attention_mask = batch['m_attention_mask']\n",
        "        diagnosis_ids = batch['diagnosis_ids']\n",
        "        diagnosis_attention_mask = batch['diagnosis_attention_mask']\n",
        "        labels = batch['labels']\n",
        "        loss, outputs = self(\n",
        "            sne_input_ids, sne_attention_mask,\n",
        "            st_input_ids, st_attention_mask,\n",
        "            p_input_ids, p_attention_mask,\n",
        "            m_input_ids, m_attention_mask,\n",
        "            diagnosis_ids, diagnosis_attention_mask, labels)\n",
        "        self.log(\"test loss\", loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "    \n",
        "    def training_epoch_end(self, outputs):\n",
        "        labels = []\n",
        "        predictions = []\n",
        "        \n",
        "        for output in outputs:\n",
        "            for out_labels in output['labels'].detach().cpu():\n",
        "                labels.append(out_labels)\n",
        "                \n",
        "            for out_predictions in output['predictions'].detach().cpu():\n",
        "                predictions.append(out_predictions)\n",
        "                \n",
        "        labels = torch.stack(labels)\n",
        "        predictions = torch.stack(predictions)\n",
        "        \n",
        "        for i, name in enumerate(Y_KEYS):\n",
        "            roc_score = auroc(predictions[:, i], labels[:, i])\n",
        "            self.logger.experiment.add_scalar(f\"{name}__roc_auc/Train\", roc_score, self.current_epoch)\n",
        "            \n",
        "    def configure_optimizers(self): \n",
        "        optimizer = AdamW(self.parameters(), lr=2e-5)\n",
        "        \n",
        "        warmup_steps = self.steps_per_epoch // 3\n",
        "        total_steps = self.steps_per_epoch * self.n_epochs - warmup_steps\n",
        "        \n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            warmup_steps,\n",
        "            total_steps\n",
        "        )\n",
        "        \n",
        "        return [optimizer], [scheduler] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0HOGiJxqTta"
      },
      "source": [
        "# classifier_model = JoinedObservationsClassifier(\n",
        "#     n_classes=len(Y_KEYS), \n",
        "#     steps_per_epoch=len(train_dataset) // BATCH_SIZE,\n",
        "#     n_epochs=N_EPOCHS\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYsqBZTHqTta"
      },
      "source": [
        "# # Try using gpus=['2']\n",
        "# trainer = pl.Trainer(max_epochs=N_EPOCHS, gpus='1', progress_bar_refresh_rate=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBroKh5kqTta"
      },
      "source": [
        "# trainer.fit(classifier_model, data_module)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYY3PSscqTta"
      },
      "source": [
        "# classifier_model_v2 = DiagnosisAndDividedObservationsClassifier(\n",
        "#     n_classes=len(Y_KEYS),\n",
        "#     n_diags=len(diagnoses_keys),\n",
        "#     steps_per_epoch=len(train_dataset) // BATCH_SIZE,\n",
        "#     n_epochs=N_EPOCHS\n",
        "# )\n",
        "# classifier_model_v2 = classifier_model_v2.cuda(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Cbm9yHBqTtb"
      },
      "source": [
        "# diag_bert_classiffier = DiagnosisAndDividedObservationsClassifierBertForDiag(\n",
        "#         n_classes=len(Y_KEYS),\n",
        "#         n_diags=len(diagnoses_keys),\n",
        "#         steps_per_epoch=len(train_dataset) // BATCH_SIZE,\n",
        "#         n_epochs=N_EPOCHS\n",
        "# )\n",
        "# diag_bert_classiffier = diag_bert_classiffier.cuda(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PExbGlPhqTtb"
      },
      "source": [
        "# extra_layer_classifier = DiagnosisAndDividedObservationsClassifierExtraLayer(\n",
        "#         n_classes=len(Y_KEYS),\n",
        "#         n_diags=len(diagnoses_keys),\n",
        "#         steps_per_epoch=len(train_dataset) // BATCH_SIZE,\n",
        "#         n_epochs=N_EPOCHS\n",
        "# )\n",
        "# extra_layer_classifier = extra_layer_classifier.cuda(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnuvTvZ2qTtb"
      },
      "source": [
        "# Try using gpus=['2']\n",
        "# trainer_v2 = pl.Trainer(max_epochs=N_EPOCHS, gpus='2', progress_bar_refresh_rate=30, checkpoint_callback=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2gteIhvqTte"
      },
      "source": [
        "# trainer_v2.fit(classifier_model_v2, data_module_200)\n",
        "# trainer_v2.save_checkpoint('/research/jamunoz/last_checkpoint_diagnosis_divided_obs_200.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOlM_cx6qTte"
      },
      "source": [
        "# trainer_v2.fit(classifier_model_v2, data_module_300)\n",
        "# trainer_v2.save_checkpoint('/research/jamunoz/last_checkpoint_diagnosis_divided_obs_300.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2QeewQHqTtf"
      },
      "source": [
        "# trainer_v2.fit(classifier_model_v2, data_module_400)\n",
        "# trainer_v2.save_checkpoint('/research/jamunoz/last_checkpoint_diagnosis_divided_obs_400.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVbG5cXfqTtf"
      },
      "source": [
        "# trainer_v2.fit(classifier_model_v2, data_module_500)\n",
        "# trainer_v2.save_checkpoint('/research/jamunoz/last_checkpoint_diagnosis_divided_obs_500.ckpt')hamming_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maByrxHNqTtf"
      },
      "source": [
        "# trainer_v2.fit(diag_bert_classiffier, data_module_200)\n",
        "# trainer_v2.save_checkpoint('/research/jamunoz/last_checkpoint_diagnosis_divided_obs_200_bert_diag.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Fp9AAheqTtf"
      },
      "source": [
        "# trainer_v2.fit(diag_bert_classiffier, data_module_300)\n",
        "# trainer_v2.save_checkpoint('/research/jamunoz/last_checkpoint_diagnosis_divided_obs_300_bert_diag.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FZhv3uVqTtf"
      },
      "source": [
        "# trainer_v2.fit(diag_bert_classiffier, data_module_400)\n",
        "# trainer_v2.save_checkpoint('/research/jamunoz/last_checkpoint_diagnosis_divided_obs_400_bert_diag.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt1HzpN3qTtf"
      },
      "source": [
        "# trainer_v2.fit(diag_bert_classiffier, data_module_500)\n",
        "# trainer_v2.save_checkpoint('/research/jamunoz/last_checkpoint_diagnosis_divided_obs_500_bert_diag.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4kcdNqSqTtg"
      },
      "source": [
        "# trainer_v2.fit(extra_layer_classifier, mini_400_module)\n",
        "# trainer_v2.save_checkpoint('/research/jamunoz/last_checkpoint_diagnosis_divided_obs_400_extra_layer.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9O9nRZ0qTtg"
      },
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OcYxNRpqTtg"
      },
      "source": [
        "def get_predictions(trained_model, test_dataset):\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    for row in test_dataset:\n",
        "        _, raw_prediction = trained_model(\n",
        "            row['sne_input_ids'].reshape(1, len(row['sne_input_ids'])),\n",
        "            row['sne_attention_mask'].reshape(1, len(row['sne_attention_mask'])),\n",
        "            row['st_input_ids'].reshape(1, len(row['st_input_ids'])),\n",
        "            row['st_attention_mask'].reshape(1, len(row['st_attention_mask'])),\n",
        "            row['p_input_ids'].reshape(1, len(row['p_input_ids'])),\n",
        "            row['p_attention_mask'].reshape(1, len(row['p_attention_mask'])),\n",
        "            row['m_input_ids'].reshape(1, len(row['m_input_ids'])),\n",
        "            row['m_attention_mask'].reshape(1, len(row['m_attention_mask'])),\n",
        "            torch.from_numpy(row['diagnosis']).reshape(1, len(row['diagnosis']))\n",
        "        )\n",
        "#         print(raw_prediction)\n",
        "        prediction = [1 if pred > 0.5 else 0 for pred in raw_prediction[0]]\n",
        "        predictions.append(prediction)\n",
        "        labels.append(row['labels'].numpy())\n",
        "    return predictions, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQcMor5sqTth"
      },
      "source": [
        "def get_text_diag_predictions(trained_model, test_dataset):\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    for row in test_dataset:\n",
        "        _, raw_prediction = trained_model(\n",
        "            row['sne_input_ids'].reshape(1, len(row['sne_input_ids'])),\n",
        "            row['sne_attention_mask'].reshape(1, len(row['sne_attention_mask'])),\n",
        "            row['st_input_ids'].reshape(1, len(row['st_input_ids'])),\n",
        "            row['st_attention_mask'].reshape(1, len(row['st_attention_mask'])),\n",
        "            row['p_input_ids'].reshape(1, len(row['p_input_ids'])),\n",
        "            row['p_attention_mask'].reshape(1, len(row['p_attention_mask'])),\n",
        "            row['m_input_ids'].reshape(1, len(row['m_input_ids'])),\n",
        "            row['m_attention_mask'].reshape(1, len(row['m_attention_mask'])),\n",
        "            row['diagnosis_ids'].reshape(1, len(row['diagnosis_ids'])),\n",
        "            row['diagnosis_attention_mask'].reshape(1, len(row['diagnosis_attention_mask'])),\n",
        "        )\n",
        "#         print(raw_prediction)\n",
        "        prediction = [1 if pred > 0.5 else 0 for pred in raw_prediction[0]]\n",
        "        predictions.append(prediction)\n",
        "        labels.append(row['labels'].numpy())\n",
        "    return predictions, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlTUQ6ICqTti"
      },
      "source": [
        "# get label of test set\n",
        "y_true = [row['labels'].numpy() for row in data_module_200.test_dataset]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtFLHIgeqTti"
      },
      "source": [
        "# trainer_v2.save_checkpoint('/research/jamunoz/last_checkpoint_diagnosis_divided_obs.ckpt')\n",
        "# classifier_model_v2 = DiagnosisAndDividedObservationsClassifier.load_from_checkpoint(\n",
        "#     '/research/jamunoz/last_checkpoint_diagnosis_divided_obs.ckpt', n_classes=len(Y_KEYS), n_diags=len(diagnoses_keys))\n",
        "# classifier_model_v2.freeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecmTnSMaqTti"
      },
      "source": [
        "# classifier_model = DiagnosisAndDividedObservationsClassifier.load_from_checkpoint(\n",
        "#     '/research/jamunoz/last_checkpoint_diagnosis_divided_obs_200.ckpt', n_classes=len(Y_KEYS), n_diags=len(diagnoses_keys))\n",
        "# classifier_model.freeze()\n",
        "# y_pred_200, y_true_200 = get_predictions(classifier_model, data_module_200.test_dataset)\n",
        "# del classifier_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je9oUmIaqTti"
      },
      "source": [
        "# classifier_model = DiagnosisAndDividedObservationsClassifier.load_from_checkpoint(\n",
        "#     '/research/jamunoz/last_checkpoint_diagnosis_divided_obs_300.ckpt', n_classes=len(Y_KEYS), n_diags=len(diagnoses_keys))\n",
        "# classifier_model.freeze()\n",
        "# y_pred_300, y_true_300 = get_predictions(classifier_model, data_module_300.test_dataset)\n",
        "# del classifier_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA4CDNLuqTtj"
      },
      "source": [
        "# classifier_model = DiagnosisAndDividedObservationsClassifier.load_from_checkpoint(\n",
        "#     '/research/jamunoz/last_checkpoint_diagnosis_divided_obs_400.ckpt', n_classes=len(Y_KEYS), n_diags=len(diagnoses_keys))\n",
        "# classifier_model.freeze()\n",
        "# y_pred_400, y_true_400 = get_predictions(classifier_model, data_module_400.test_dataset)\n",
        "# del classifier_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ShyAVqnqTtj"
      },
      "source": [
        "# classifier_model = DiagnosisAndDividedObservationsClassifier.load_from_checkpoint(\n",
        "#     '/research/jamunoz/last_checkpoint_diagnosis_divided_obs_500.ckpt', n_classes=len(Y_KEYS), n_diags=len(diagnoses_keys))\n",
        "# classifier_model.freeze()\n",
        "# y_pred_500, y_true_500 = get_predictions(classifier_model, data_module_500.test_dataset)\n",
        "# del classifier_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35JYLo9sqTtj"
      },
      "source": [
        "# classifier_model = DiagnosisAndDividedObservationsClassifierBertForDiag.load_from_checkpoint(\n",
        "#     '/research/jamunoz/last_checkpoint_diagnosis_divided_obs_200_bert_diag.ckpt', n_classes=len(Y_KEYS), n_diags=len(diagnoses_keys))\n",
        "# classifier_model.freeze()\n",
        "# y_pred_200_bert_diag, _ = get_text_diag_predictions(classifier_model, data_module_200.test_dataset)\n",
        "# del classifier_model\n",
        "# with open('preds_200_bert_diag.npy', 'wb') as f:\n",
        "#     np.save(f, y_pred_200_bert_diag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL-ezk0IqTtj"
      },
      "source": [
        "# classifier_model = DiagnosisAndDividedObservationsClassifierBertForDiag.load_from_checkpoint(\n",
        "#     '/research/jamunoz/last_checkpoint_diagnosis_divided_obs_300_bert_diag.ckpt', n_classes=len(Y_KEYS), n_diags=len(diagnoses_keys))\n",
        "# classifier_model.freeze()\n",
        "# y_pred_300_bert_diag, _ = get_text_diag_predictions(classifier_model, data_module_300.test_dataset)\n",
        "# del classifier_model\n",
        "# with open('preds_300_bert_diag.npy', 'wb') as f:\n",
        "#     np.save(f, y_pred_300_bert_diag)\n",
        "    \n",
        "# classifier_model = DiagnosisAndDividedObservationsClassifierBertForDiag.load_from_checkpoint(\n",
        "#     '/research/jamunoz/last_checkpoint_diagnosis_divided_obs_400_bert_diag.ckpt', n_classes=len(Y_KEYS), n_diags=len(diagnoses_keys))\n",
        "# classifier_model.freeze()\n",
        "# y_pred_400_bert_diag, _ = get_text_diag_predictions(classifier_model, data_module_400.test_dataset)\n",
        "# del classifier_model\n",
        "# with open('preds_400_bert_diag.npy', 'wb') as f:\n",
        "#     np.save(f, y_pred_400_bert_diag)\n",
        "    \n",
        "# classifier_model = DiagnosisAndDividedObservationsClassifierBertForDiag.load_from_checkpoint(\n",
        "#     '/research/jamunoz/last_checkpoint_diagnosis_divided_obs_500_bert_diag.ckpt', n_classes=len(Y_KEYS), n_diags=len(diagnoses_keys))\n",
        "# classifier_model.freeze()\n",
        "# y_pred_500_bert_diag, _ = get_text_diag_predictions(classifier_model, data_module_500.test_dataset)\n",
        "# del classifier_model\n",
        "# with open('preds_500_bert_diag.npy', 'wb') as f:\n",
        "#     np.save(f, y_pred_500_bert_diag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbjHW_mBqTtj"
      },
      "source": [
        "# print('haciendo 200')\n",
        "# classifier_model = DiagnosisAndDividedObservationsClassifierExtraLayer.load_from_checkpoint(\n",
        "#     '/research/jamunoz/last_checkpoint_diagnosis_divided_obs_200_extra_layer.ckpt', n_classes=len(Y_KEYS), n_diags=len(diagnoses_keys))\n",
        "# classifier_model.freeze()\n",
        "# y_pred_200_extra_layer, _ = get_predictions(classifier_model, data_module_200.test_dataset)\n",
        "# del classifier_model\n",
        "# with open('preds_200_extra_layer.npy', 'wb') as f:\n",
        "#     np.save(f, y_pred_200_extra_layer)\n",
        "\n",
        "# print('haciendo 300')\n",
        "# classifier_model = DiagnosisAndDividedObservationsClassifierExtraLayer.load_from_checkpoint(\n",
        "#     '/research/jamunoz/last_checkpoint_diagnosis_divided_obs_300_extra_layer.ckpt', n_classes=len(Y_KEYS), n_diags=len(diagnoses_keys))\n",
        "# classifier_model.freeze()\n",
        "# y_pred_300_extra_layer, _ = get_predictions(classifier_model, data_module_300.test_dataset)\n",
        "# del classifier_model\n",
        "# with open('preds_300_extra_layer.npy', 'wb') as f:\n",
        "#     np.save(f, y_pred_300_extra_layer)\n",
        "\n",
        "# print('haciendo 400')\n",
        "# classifier_model = DiagnosisAndDividedObservationsClassifierExtraLayer.load_from_checkpoint(\n",
        "#     '/research/jamunoz/last_checkpoint_diagnosis_divided_obs_400_extra_layer.ckpt', n_classes=len(Y_KEYS), n_diags=len(diagnoses_keys))\n",
        "# classifier_model.freeze()\n",
        "# y_pred_400_extra_layer, _ = get_predictions(classifier_model, data_module_400.test_dataset)\n",
        "# del classifier_model\n",
        "# with open('preds_400_extra_layer.npy', 'wb') as f:\n",
        "#     np.save(f, y_pred_400_extra_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYhHFfa2qTtk"
      },
      "source": [
        "# with open('preds_200.npy', 'wb') as f:\n",
        "#     np.save(f, y_pred_200)\n",
        "y_pred_200 = np.load('preds_200.npy')\n",
        "\n",
        "# with open('preds_300.npy', 'wb') as x:\n",
        "#     np.save(x, y_pred_300)\n",
        "y_pred_300 = np.load('preds_300.npy')\n",
        "\n",
        "# with open('preds_400.npy', 'wb') as y:\n",
        "#     np.save(y, y_pred_400)\n",
        "y_pred_400 = np.load('preds_400.npy')\n",
        "\n",
        "# with open('preds_500.npy', 'wb') as y:\n",
        "#     np.save(y, y_pred_500)\n",
        "y_pred_500 = np.load('preds_500.npy')\n",
        "\n",
        "y_pred_200_bert_diag = np.load('preds_200_bert_diag.npy')\n",
        "\n",
        "y_pred_300_bert_diag = np.load('preds_300_bert_diag.npy')\n",
        "\n",
        "y_pred_400_bert_diag = np.load('preds_400_bert_diag.npy')\n",
        "\n",
        "y_pred_500_bert_diag = np.load('preds_500_bert_diag.npy')\n",
        "\n",
        "\n",
        "y_pred_200_extra_layer = np.load('preds_200_extra_layer.npy')\n",
        "\n",
        "y_pred_300_extra_layer = np.load('preds_300_extra_layer.npy')\n",
        "\n",
        "y_pred_400_extra_layer = np.load('preds_400_extra_layer.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki5g1COMqTtk"
      },
      "source": [
        "def comp(arr_of_arr1, arr_of_arr2):\n",
        "    if len(arr_of_arr1) != len(arr_of_arr2):\n",
        "        return False\n",
        "    else:\n",
        "        for (i, arr) in enumerate(arr_of_arr1):\n",
        "            if len(arr) != len(arr_of_arr2[i]):\n",
        "                return False\n",
        "            else:\n",
        "                for (j, el) in enumerate(arr):\n",
        "                    if el != arr_of_arr2[i][j]:\n",
        "                        return False\n",
        "    return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF5d1QE3qTtk"
      },
      "source": [
        "print('max token 200 OHE diagnosis Single BETO')\n",
        "print('Accuracy: ', accuracy_score(y_true, y_pred_200))\n",
        "print('Micro F1: ', f1_score(y_true, y_pred_200, average='micro'))\n",
        "print('Macro F1: ', f1_score(y_true, y_pred_200, average='macro'))\n",
        "print('Weighted F1: ', f1_score(y_true, y_pred_200, average='weighted'))\n",
        "print('Hamming Loss: ', hamming_loss(y_true, y_pred_200))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UmdfQEOqTtl"
      },
      "source": [
        "print('max token 300 OHE diagnosis Single BETO')\n",
        "print('Accuracy: ', accuracy_score(y_true, y_pred_300))\n",
        "print('Micro F1: ', f1_score(y_true, y_pred_300, average='micro'))\n",
        "print('Macro F1: ', f1_score(y_true, y_pred_300, average='macro'))\n",
        "print('Weighted F1: ', f1_score(y_true, y_pred_300, average='weighted'))\n",
        "print('Hamming Loss: ', hamming_loss(y_true, y_pred_300))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLf0BYOAqTtl"
      },
      "source": [
        "print('max token 400 OHE diagnosis Single BETO')\n",
        "print('Accuracy: ', accuracy_score(y_true, y_pred_400))\n",
        "print('Micro F1: ', f1_score(y_true, y_pred_400, average='micro'))\n",
        "print('Macro F1: ', f1_score(y_true, y_pred_400, average='macro'))\n",
        "print('Weighted F1: ', f1_score(y_true, y_pred_400, average='weighted'))\n",
        "print('Hamming Loss: ', hamming_loss(y_true, y_pred_400))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcoI3aytqTtl"
      },
      "source": [
        "print('max token 500 OHE diagnosis Single BETO')\n",
        "print('Accuracy: ', accuracy_score(y_true, y_pred_500))\n",
        "print('Micro F1: ', f1_score(y_true, y_pred_500, average='micro'))\n",
        "print('Macro F1: ', f1_score(y_true, y_pred_500, average='macro'))\n",
        "print('Weighted F1: ', f1_score(y_true, y_pred_500, average='weighted'))\n",
        "print('Hamming Loss: ', hamming_loss(y_true, y_pred_500))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJalH6dUqTtm"
      },
      "source": [
        "print('max token 200 BETO diagnosis Single BETO')\n",
        "print('Accuracy: ', accuracy_score(y_true, y_pred_200_bert_diag))\n",
        "print('Micro F1: ', f1_score(y_true, y_pred_200_bert_diag, average='micro'))\n",
        "print('Macro F1: ', f1_score(y_true, y_pred_200_bert_diag, average='macro'))\n",
        "print('Weighted F1: ', f1_score(y_true, y_pred_200_bert_diag, average='weighted'))\n",
        "print('Hamming Loss: ', hamming_loss(y_true, y_pred_200_bert_diag))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjAXBnv1qTtm"
      },
      "source": [
        "print('max token 300 BETO diagnosis Single BETO')\n",
        "print('Accuracy: ', accuracy_score(y_true, y_pred_300_bert_diag))\n",
        "print('Micro F1: ', f1_score(y_true, y_pred_300_bert_diag, average='micro'))\n",
        "print('Macro F1: ', f1_score(y_true, y_pred_300_bert_diag, average='macro'))\n",
        "print('Weighted F1: ', f1_score(y_true, y_pred_300_bert_diag, average='weighted'))\n",
        "print('Hamming Loss: ', hamming_loss(y_true, y_pred_300_bert_diag))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8d5KfFWqTtm"
      },
      "source": [
        "print('max token 400 BETO diagnosis Single BETO')\n",
        "print('Accuracy: ', accuracy_score(y_true, y_pred_400_bert_diag))\n",
        "print('Micro F1: ', f1_score(y_true, y_pred_400_bert_diag, average='micro'))\n",
        "print('Macro F1: ', f1_score(y_true, y_pred_400_bert_diag, average='macro'))\n",
        "print('Weighted F1: ', f1_score(y_true, y_pred_400_bert_diag, average='weighted'))\n",
        "print('Hamming Loss: ', hamming_loss(y_true, y_pred_400_bert_diag))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDqvWe2rqTtn"
      },
      "source": [
        "print('max token 500 BETO diagnosis Single BETO')\n",
        "print('Accuracy: ', accuracy_score(y_true, y_pred_500_bert_diag))\n",
        "print('Micro F1: ', f1_score(y_true, y_pred_500_bert_diag, average='micro'))\n",
        "print('Macro F1: ', f1_score(y_true, y_pred_500_bert_diag, average='macro'))\n",
        "print('Weighted F1: ', f1_score(y_true, y_pred_500_bert_diag, average='weighted'))\n",
        "print('Hamming Loss: ', hamming_loss(y_true, y_pred_500_bert_diag))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfcXHTJFqTt3"
      },
      "source": [
        "print('max token 200 OHE diagnosis Single BETO Additional layer')\n",
        "print('Accuracy: ', accuracy_score(y_true, y_pred_200_extra_layer))\n",
        "print('Micro F1: ', f1_score(y_true, y_pred_200_extra_layer, average='micro'))\n",
        "print('Macro F1: ', f1_score(y_true, y_pred_200_extra_layer, average='macro'))\n",
        "print('Weighted F1: ', f1_score(y_true, y_pred_200_extra_layer, average='weighted'))\n",
        "print('Hamming Loss: ', hamming_loss(y_true, y_pred_200_extra_layer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3n88L5fqTt4"
      },
      "source": [
        "print('max token 300 OHE diagnosis Single BETO Additional layer')\n",
        "print('Accuracy: ', accuracy_score(y_true, y_pred_300_extra_layer))\n",
        "print('Micro F1: ', f1_score(y_true, y_pred_300_extra_layer, average='micro'))\n",
        "print('Macro F1: ', f1_score(y_true, y_pred_300_extra_layer, average='macro'))\n",
        "print('Weighted F1: ', f1_score(y_true, y_pred_300_extra_layer, average='weighted'))\n",
        "print('Hamming Loss: ', hamming_loss(y_true, y_pred_300_extra_layer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuN0v4fVqTt4"
      },
      "source": [
        "print('max token 400 OHE diagnosis Single BETO Additional layer')\n",
        "print('Accuracy: ', accuracy_score(y_true, y_pred_400_extra_layer))\n",
        "print('Micro F1: ', f1_score(y_true, y_pred_400_extra_layer, average='micro'))\n",
        "print('Macro F1: ', f1_score(y_true, y_pred_400_extra_layer, average='macro'))\n",
        "print('Weighted F1: ', f1_score(y_true, y_pred_400_extra_layer, average='weighted'))\n",
        "print('Hamming Loss: ', hamming_loss(y_true, y_pred_400_extra_layer))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}